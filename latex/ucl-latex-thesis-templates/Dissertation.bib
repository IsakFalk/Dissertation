
@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2017-06-21},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{neubig_neural_2017,
	title = {Neural {Machine} {Translation} and {Sequence}-to-sequence {Models}: {A} {Tutorial}},
	shorttitle = {Neural {Machine} {Translation} and {Sequence}-to-sequence {Models}},
	url = {http://arxiv.org/abs/1703.01619},
	abstract = {This tutorial introduces a new and powerful set of techniques variously called "neural machine translation" or "neural sequence-to-sequence models". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.},
	urldate = {2017-06-21},
	journal = {arXiv:1703.01619 [cs, stat]},
	author = {Neubig, Graham},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01619},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{cheng_long_2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	urldate = {2017-06-21},
	journal = {arXiv:1601.06733 [cs]},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06733},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2017-06-21},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{yang_improved_2017,
	title = {Improved {Variational} {Autoencoders} for {Text} {Modeling} using {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1702.08139},
	abstract = {Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
	urldate = {2017-06-21},
	journal = {arXiv:1702.08139 [cs]},
	author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08139},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{johnson_googles_2016,
	title = {Google's {Multilingual} {Neural} {Machine} {Translation} {System}: {Enabling} {Zero}-{Shot} {Translation}},
	shorttitle = {Google's {Multilingual} {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1611.04558},
	abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English\${\textbackslash}rightarrow\$French and surpasses state-of-the-art results for English\${\textbackslash}rightarrow\$German. Similarly, a single multilingual model surpasses state-of-the-art results for French\${\textbackslash}rightarrow\$English and German\${\textbackslash}rightarrow\$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
	urldate = {2017-06-21},
	journal = {arXiv:1611.04558 [cs]},
	author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi√©gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04558},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2017-06-21},
	journal = {arXiv:1310.4546 [cs, stat]},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.4546},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{zhou_deep_2016,
	title = {Deep {Recurrent} {Models} with {Fast}-{Forward} {Connections} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1606.04199},
	abstract = {Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.},
	urldate = {2017-06-21},
	journal = {arXiv:1606.04199 [cs]},
	author = {Zhou, Jie and Cao, Ying and Wang, Xuguang and Li, Peng and Xu, Wei},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04199},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@article{hu_controllable_2017,
	title = {Controllable {Text} {Generation}},
	url = {http://arxiv.org/abs/1703.00955},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	urldate = {2017-06-21},
	journal = {arXiv:1703.00955 [cs, stat]},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00955},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2017-06-21},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{wu_adversarial_2017,
	title = {Adversarial {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1704.06933},
	abstract = {In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by a NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English\${\textbackslash}rightarrow\$French and German\${\textbackslash}rightarrow\$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.},
	urldate = {2017-06-21},
	journal = {arXiv:1704.06933 [cs, stat]},
	author = {Wu, Lijun and Xia, Yingce and Zhao, Li and Tian, Fei and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06933},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{gehring_convolutional_2016,
	title = {A {Convolutional} {Encoder} {Model} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1611.02344},
	abstract = {The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.},
	urldate = {2017-06-21},
	journal = {arXiv:1611.02344 [cs]},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02344},
	keywords = {Computer Science - Computation and Language}
}

@article{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://arxiv.org/abs/1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	urldate = {2017-06-21},
	journal = {arXiv:1302.4389 [cs, stat]},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4389},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{wu_googles_2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1609.08144},
	abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
	urldate = {2017-06-21},
	journal = {arXiv:1609.08144 [cs]},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, ≈Åukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08144},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning}
}

@article{graves_generating_2013,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2017-06-21},
	journal = {arXiv:1308.0850 [cs]},
	author = {Graves, Alex},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.0850},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@article{bowman_generating_2015,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	urldate = {2017-06-21},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2017-06-21},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language}
}

@article{semeniuta_hybrid_2017,
	title = {A {Hybrid} {Convolutional} {Variational} {Autoencoder} for {Text} {Generation}},
	url = {http://arxiv.org/abs/1702.02390},
	abstract = {In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.},
	urldate = {2017-06-21},
	journal = {arXiv:1702.02390 [cs]},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.02390},
	keywords = {Computer Science - Computation and Language}
}

@article{mescheder_adversarial_2017,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2017-06-21},
	journal = {arXiv:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04722},
	keywords = {Computer Science - Learning}
}

@article{luong_achieving_2016,
	title = {Achieving {Open} {Vocabulary} {Neural} {Machine} {Translation} with {Hybrid} {Word}-{Character} {Models}},
	url = {http://arxiv.org/abs/1604.00788},
	abstract = {Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.},
	urldate = {2017-06-21},
	journal = {arXiv:1604.00788 [cs]},
	author = {Luong, Minh-Thang and Manning, Christopher D.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00788},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@incollection{Rumelhart:1995:BBT:201784.201785,
 author = {Rumelhart, David E. and Durbin, Richard and Golden, Richard and Chauvin, Yves},
 chapter = {Backpropagation: The Basic Theory},
 title = {Backpropagation},
 editor = {Chauvin, Yves and Rumelhart, David E.},
 year = {1995},
 isbn = {0-8058-1259-8},
 pages = {1--34},
 numpages = {34},
 url = {http://dl.acm.org/citation.cfm?id=201784.201785},
 acmid = {201785},
 publisher = {L. Erlbaum Associates Inc.},
 address = {Hillsdale, NJ, USA},
} 

@Book{Bishop:2006,
  author =       "Bishop, Christopher M.",
  title =        "Pattern Recognition and Machine Learning",
  publisher =    "Springer",
  year =         "2006",
  ISBN =         "978-0387-31073-2",
  url = "http://research.microsoft.com/en-us/um/people/cmbishop/prml/",
  bib2html_rescat = "General ML",
}

@inproceedings{koehn2005epc,
  added-at = {2010-11-10T15:46:05.000+0100},
  address = {Phuket, Thailand},
  author = {Koehn, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/205e5fa13fd7ba7992aedfe3514379a1f/unhammer},
  booktitle = {{Conference Proceedings: the tenth Machine Translation Summit}},
  interhash = {abd06b551527865eb50e21508841b6de},
  intrahash = {05e5fa13fd7ba7992aedfe3514379a1f},
  keywords = {Corpus Europarl MT Master},
  organization = {AAMT},
  pages = {79--86},
  publisher = {AAMT},
  timestamp = {2010-11-10T15:46:05.000+0100},
  title = {{Europarl: A Parallel Corpus for Statistical Machine Translation}},
  url = {http://mt-archive.info/MTS-2005-Koehn.pdf},
  year = 2005
}
