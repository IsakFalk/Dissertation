
@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2017-06-21},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{neubig_neural_2017,
	title = {Neural {Machine} {Translation} and {Sequence}-to-sequence {Models}: {A} {Tutorial}},
	shorttitle = {Neural {Machine} {Translation} and {Sequence}-to-sequence {Models}},
	url = {http://arxiv.org/abs/1703.01619},
	abstract = {This tutorial introduces a new and powerful set of techniques variously called "neural machine translation" or "neural sequence-to-sequence models". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.},
	urldate = {2017-06-21},
	journal = {arXiv:1703.01619 [cs, stat]},
	author = {Neubig, Graham},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01619},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{cheng_long_2016,
	title = {Long {Short}-{Term} {Memory}-{Networks} for {Machine} {Reading}},
	url = {http://arxiv.org/abs/1601.06733},
	abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
	urldate = {2017-06-21},
	journal = {arXiv:1601.06733 [cs]},
	author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.06733},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2017-06-21},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{yang_improved_2017,
	title = {Improved {Variational} {Autoencoders} for {Text} {Modeling} using {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1702.08139},
	abstract = {Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
	urldate = {2017-06-21},
	journal = {arXiv:1702.08139 [cs]},
	author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08139},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{johnson_googles_2016,
	title = {Google's {Multilingual} {Neural} {Machine} {Translation} {System}: {Enabling} {Zero}-{Shot} {Translation}},
	shorttitle = {Google's {Multilingual} {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1611.04558},
	abstract = {We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English\${\textbackslash}rightarrow\$French and surpasses state-of-the-art results for English\${\textbackslash}rightarrow\$German. Similarly, a single multilingual model surpasses state-of-the-art results for French\${\textbackslash}rightarrow\$English and German\${\textbackslash}rightarrow\$English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.},
	urldate = {2017-06-21},
	journal = {arXiv:1611.04558 [cs]},
	author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi√©gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04558},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2017-06-21},
	journal = {arXiv:1310.4546 [cs, stat]},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.4546},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{zhou_deep_2016,
	title = {Deep {Recurrent} {Models} with {Fast}-{Forward} {Connections} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1606.04199},
	abstract = {Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.},
	urldate = {2017-06-21},
	journal = {arXiv:1606.04199 [cs]},
	author = {Zhou, Jie and Cao, Ying and Wang, Xuguang and Li, Peng and Xu, Wei},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04199},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@article{hu_controllable_2017,
	title = {Controllable {Text} {Generation}},
	url = {http://arxiv.org/abs/1703.00955},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	urldate = {2017-06-21},
	journal = {arXiv:1703.00955 [cs, stat]},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00955},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2017-06-21},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{wu_adversarial_2017,
	title = {Adversarial {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1704.06933},
	abstract = {In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by a NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English\${\textbackslash}rightarrow\$French and German\${\textbackslash}rightarrow\$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.},
	urldate = {2017-06-21},
	journal = {arXiv:1704.06933 [cs, stat]},
	author = {Wu, Lijun and Xia, Yingce and Zhao, Li and Tian, Fei and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06933},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning}
}

@article{gehring_convolutional_2016,
	title = {A {Convolutional} {Encoder} {Model} for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1611.02344},
	abstract = {The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. In this paper we present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. Our convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline.},
	urldate = {2017-06-21},
	journal = {arXiv:1611.02344 [cs]},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02344},
	keywords = {Computer Science - Computation and Language}
}

@article{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://arxiv.org/abs/1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	urldate = {2017-06-21},
	journal = {arXiv:1302.4389 [cs, stat]},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {arXiv: 1302.4389},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{wu_googles_2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1609.08144},
	abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
	urldate = {2017-06-21},
	journal = {arXiv:1609.08144 [cs]},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, ≈Åukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08144},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Learning}
}

@article{graves_generating_2013,
	title = {Generating {Sequences} {With} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2017-06-21},
	journal = {arXiv:1308.0850 [cs]},
	author = {Graves, Alex},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.0850},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@article{bowman_generating_2015,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	urldate = {2017-06-21},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2017-06-21},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language}
}

@article{semeniuta_hybrid_2017,
	title = {A {Hybrid} {Convolutional} {Variational} {Autoencoder} for {Text} {Generation}},
	url = {http://arxiv.org/abs/1702.02390},
	abstract = {In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.},
	urldate = {2017-06-21},
	journal = {arXiv:1702.02390 [cs]},
	author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.02390},
	keywords = {Computer Science - Computation and Language}
}

@article{mescheder_adversarial_2017,
	title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
	shorttitle = {Adversarial {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1701.04722},
	abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	urldate = {2017-06-21},
	journal = {arXiv:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04722},
	keywords = {Computer Science - Learning}
}

@article{luong_achieving_2016,
	title = {Achieving {Open} {Vocabulary} {Neural} {Machine} {Translation} with {Hybrid} {Word}-{Character} {Models}},
	url = {http://arxiv.org/abs/1604.00788},
	abstract = {Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.},
	urldate = {2017-06-21},
	journal = {arXiv:1604.00788 [cs]},
	author = {Luong, Minh-Thang and Manning, Christopher D.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00788},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning}
}

@incollection{Rumelhart:1995:BBT:201784.201785,
 author = {Rumelhart, David E. and Durbin, Richard and Golden, Richard and Chauvin, Yves},
 chapter = {Backpropagation: The Basic Theory},
 title = {Backpropagation},
 editor = {Chauvin, Yves and Rumelhart, David E.},
 year = {1995},
 isbn = {0-8058-1259-8},
 pages = {1--34},
 numpages = {34},
 url = {http://dl.acm.org/citation.cfm?id=201784.201785},
 acmid = {201785},
 publisher = {L. Erlbaum Associates Inc.},
 address = {Hillsdale, NJ, USA},
} 

@Book{Bishop:2006,
  author =       "Bishop, Christopher M.",
  title =        "Pattern Recognition and Machine Learning",
  publisher =    "Springer",
  year =         "2006",
  ISBN =         "978-0387-31073-2",
  url = "http://research.microsoft.com/en-us/um/people/cmbishop/prml/",
  bib2html_rescat = "General ML",
}

@inproceedings{koehn2005epc,
  added-at = {2010-11-10T15:46:05.000+0100},
  address = {Phuket, Thailand},
  author = {Koehn, Philipp},
  biburl = {https://www.bibsonomy.org/bibtex/205e5fa13fd7ba7992aedfe3514379a1f/unhammer},
  booktitle = {{Conference Proceedings: the tenth Machine Translation Summit}},
  interhash = {abd06b551527865eb50e21508841b6de},
  intrahash = {05e5fa13fd7ba7992aedfe3514379a1f},
  keywords = {Corpus Europarl MT Master},
  organization = {AAMT},
  pages = {79--86},
  publisher = {AAMT},
  timestamp = {2010-11-10T15:46:05.000+0100},
  title = {{Europarl: A Parallel Corpus for Statistical Machine Translation}},
  url = {http://mt-archive.info/MTS-2005-Koehn.pdf},
  year = 2005
}

@book{jaynes2003probability,
  title={Probability Theory: The Logic of Science},
  author={Jaynes, E.T. and Bretthorst, G.L.},
  isbn={9780521592710},
  lccn={2002071486},
  url={https://books.google.co.uk/books?id=tTN4HuUNXjgC},
  year={2003},
  publisher={Cambridge University Press}
}

@book{Press:2007:NRE:1403886,
 author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
 title = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
 year = {2007},
 isbn = {0521880688, 9780521880688},
 edition = {3},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 

@book{CaseBerg:01,
  abstract = {{This book builds theoretical statistics from the first
		  principles of probability theory. Starting from the basics
		  of probability, the authors develop the theory of
		  statistical inference using techniques, definitions, and
		  concepts that are statistical and are natural extensions
		  and consequences of previous concepts. Intended for
		  first-year graduate students, this book can be used for
		  students majoring in statistics who have a solid
		  mathematics background. It can also be used in a way that
		  stresses the more practical uses of statistical theory,
		  being more concerned with understanding basic statistical
		  concepts and deriving reasonable statistical procedures for
		  a variety of situations, and less concerned with formal
		  optimality investigations.}},
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {Casella, George and Berger, Roger},
  biburl = {https://www.bibsonomy.org/bibtex/21597678f36e23439610affbf46adec1c/jwbowers},
  citeulike-article-id = {105644},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  howpublished = {{Textbook Binding}},
  interhash = {2dd8caad6c0b6fb80e6334986a231a05},
  intrahash = {1597678f36e23439610affbf46adec1c},
  isbn = {0534243126},
  keywords = {methodology probability statistics},
  month = {June},
  opturl = {http://www.amazon.fr/exec/obidos/ASIN/0534243126/citeulike04-21},
  publisher = {{Duxbury Resource Center}},
  timestamp = {2009-10-28T04:42:57.000+0100},
  title = {Statistical Inference},
  year = 2001
}

@book{brooks2011handbook,
  added-at = {2015-03-24T17:28:34.000+0100},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  biburl = {https://www.bibsonomy.org/bibtex/22b8d02bec832fa945b62ecf7808614bf/becker},
  interhash = {0b127e40d41a970274484b65a7e0744f},
  intrahash = {2b8d02bec832fa945b62ecf7808614bf},
  keywords = {carlo chain handbook markov mcmc monte},
  publisher = {CRC press},
  timestamp = {2015-03-24T17:33:19.000+0100},
  title = {Handbook of Markov Chain Monte Carlo},
  url = {https://scholar.google.de/scholar.bib?q=info:A4xlUax02S0J:scholar.google.com/&output=citation&hl=de&ct=citation&cd=0},
  year = 2011
}

@MISC{Carpenter_stan:a,
    author = {Bob Carpenter and Daniel Lee and Marcus A. Brubaker and Allen Riddell and Andrew Gelman and Ben Goodrich and Jiqiang Guo and Matt Hoffman and Michael Betancourt and Peter Li},
    title = {Stan: A Probabilistic Programming Language},
    year = {}
}

@article{journals/peerj-cs/SalvatierWF16,
  added-at = {2016-06-22T00:00:00.000+0200},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  biburl = {https://www.bibsonomy.org/bibtex/26ab34e5d272f050aa380db3f6465723f/dblp},
  ee = {http://dx.doi.org/10.7717/peerj-cs.55},
  interhash = {eef07a238aeeac284294fd85d15fb5d2},
  intrahash = {6ab34e5d272f050aa380db3f6465723f},
  journal = {PeerJ Computer Science},
  keywords = {dblp},
  pages = {e55},
  timestamp = {2016-06-23T11:38:20.000+0200},
  title = {Probabilistic programming in Python using PyMC3.},
  url = {http://dblp.uni-trier.de/db/journals/peerj-cs/peerj-cs2.html#SalvatierWF16},
  volume = 2,
  year = 2016
}

@ARTICLE{Dempster77maximumlikelihood,
    author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
    title = {Maximum likelihood from incomplete data via the EM algorithm},
    journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
    year = {1977},
    volume = {39},
    number = {1},
    pages = {1--38}
}

@INPROCEEDINGS{Neal98aview,
    author = {Radford Neal and Geoffrey E. Hinton},
    title = {A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants},
    booktitle = {Learning in Graphical Models},
    year = {1998},
    pages = {355--368},
    publisher = {Kluwer Academic Publishers}
}

@article{bengio_practical_2012,
	title = {Practical recommendations for gradient-based training of deep architectures},
	url = {http://arxiv.org/abs/1206.5533},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	urldate = {2017-07-30},
	journal = {arXiv:1206.5533 [cs]},
	author = {Bengio, Yoshua},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5533},
	keywords = {Computer Science - Learning}
}

@article{Larochelle:2009:EST:1577069.1577070,
 author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Lamblin, Pascal},
 title = {Exploring Strategies for Training Deep Neural Networks},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = jun,
 year = {2009},
 issn = {1532-4435},
 pages = {1--40},
 numpages = {40},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1577070},
 acmid = {1577070},
 publisher = {JMLR.org},
}

@phdthesis{beal2003,
  added-at = {2010-03-25T16:34:19.000+0100},
  author = {Beal, Matthew J.},
  biburl = {https://www.bibsonomy.org/bibtex/223a0ca246a6d81fe92a70bbcda7dc1fb/3mta3},
  file = {beal2003.pdf:Papers/beal2003.pdf:PDF},
  interhash = {c7675c921755e23c8cc2377c0c0c387c},
  intrahash = {23a0ca246a6d81fe92a70bbcda7dc1fb},
  keywords = {Variationalmethods},
  school = {Gatsby Computational Neuroscience Unit, University College London},
  timestamp = {2010-03-25T16:34:19.000+0100},
  title = {Variational Algorithms for Approximate Bayesian Inference},
  url = {http://www.cse.buffalo.edu/faculty/mbeal/thesis/index.html},
  year = 2003
}


@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2017-08-03},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning}
}

@article{mandt_stochastic_2017,
	title = {Stochastic {Gradient} {Descent} as {Approximate} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/1704.04289},
	abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
	urldate = {2017-08-03},
	journal = {arXiv:1704.04289 [cs, stat]},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04289},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{bottou_optimization_2016,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.04838},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	urldate = {2017-08-03},
	journal = {arXiv:1606.04838 [cs, math, stat]},
	author = {Bottou, L√©on and Curtis, Frank E. and Nocedal, Jorge},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04838},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning}
}

@article{choromanska_loss_2014,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	url = {http://arxiv.org/abs/1412.0233},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	urldate = {2017-08-03},
	journal = {arXiv:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G√©rard Ben and LeCun, Yann},
	month = nov,
	year = {2014},
	note = {arXiv: 1412.0233},
	keywords = {Computer Science - Learning}
}

@article{bengio_practical_2012,
	title = {Practical recommendations for gradient-based training of deep architectures},
	url = {http://arxiv.org/abs/1206.5533},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	urldate = {2017-07-30},
	journal = {arXiv:1206.5533 [cs]},
	author = {Bengio, Yoshua},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5533},
	keywords = {Computer Science - Learning}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2017-07-29},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877}
}

@article{roeder_sticking_2017,
	title = {Sticking the {Landing}: {Simple}, {Lower}-{Variance} {Gradient} {Estimators} for {Variational} {Inference}},
	shorttitle = {Sticking the {Landing}},
	url = {http://arxiv.org/abs/1703.09194},
	abstract = {We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.},
	urldate = {2017-07-06},
	journal = {arXiv:1703.09194 [cs, stat]},
	author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.09194},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@techreport{Duchi:EECS-2010-24,
    Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
    Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2010},
    Month = {Mar},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html},
    Number = {UCB/EECS-2010-24},
    Abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods significantly outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
}

@misc{Tieleman2012,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@misc{Ruder17,
Author = {Sebastian Ruder},
Title = {An overview of gradient descent optimization algorithms},
Year = {2016},
Eprint = {arXiv:1609.04747},
}

@incollection{series/lncs/Bottou12,
  added-at = {2013-09-09T00:00:00.000+0200},
  author = {Bottou, L√©on},
  biburl = {https://www.bibsonomy.org/bibtex/214e7bfe672b9932edc8647cad86d2a6e/dblp},
  booktitle = {Neural Networks: Tricks of the Trade (2nd ed.)},
  crossref = {series/lncs/7700},
  editor = {Montavon, Gr√©goire and Orr, Genevieve B. and M√ºller, Klaus-Robert},
  ee = {http://dx.doi.org/10.1007/978-3-642-35289-8_25},
  interhash = {ac942fcc1c7d5482cfb5d4e136ada207},
  intrahash = {14e7bfe672b9932edc8647cad86d2a6e},
  isbn = {978-3-642-35288-1},
  keywords = {dblp},
  pages = {421-436},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2013-09-10T11:35:16.000+0200},
  title = {Stochastic Gradient Descent Tricks.},
  url = {http://dblp.uni-trier.de/db/series/lncs/lncs7700.html#Bottou12},
  volume = 7700,
  year = 2012
}

@article{mandt_variational_2016,
	title = {A {Variational} {Analysis} of {Stochastic} {Gradient} {Algorithms}},
	url = {http://arxiv.org/abs/1602.02666},
	abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.},
	urldate = {2017-08-04},
	journal = {arXiv:1602.02666 [cs, stat]},
	author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02666},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@INPROCEEDINGS{Rosenfeld00twodecades,
    author = {Ronald Rosenfeld},
    title = {Two decades of statistical language modeling: Where do we go from here},
    booktitle = {Proceedings of the IEEE},
    year = {2000},
    pages = {2000}
}

@InCollection{sep-computational-linguistics,
	author       =	{Schubert, Lenhart},
	title        =	{Computational Linguistics},
	booktitle    =	{The Stanford Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2015/entries/computational-linguistics/}},
	year         =	{2015},
	edition      =	{Spring 2015},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@incollection{Hinton:1986:DR:104279.104287,
 author = {Hinton, G. E. and McClelland, J. L. and Rumelhart, D. E.},
 chapter = {Distributed Representations},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {77--109},
 numpages = {33},
 url = {http://dl.acm.org/citation.cfm?id=104279.104287},
 acmid = {104287},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Bengio:2003:NPL:944919.944966,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 title = {A Neural Probabilistic Language Model},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {1137--1155},
 numpages = {19},
 url = {http://dl.acm.org/citation.cfm?id=944919.944966},
 acmid = {944966},
 publisher = {JMLR.org},
}

@article{DBLP:journals/corr/abs-1301-3781,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Wed, 07 Jun 2017 14:42:25 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Mikolov:2013:DRW:2999792.2999959,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 title = {Distributed Representations of Words and Phrases and Their Compositionality},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {3111--3119},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
 acmid = {2999959},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@INPROCEEDINGS{Pennington14glove:global,
    author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
    title = {Glove: Global vectors for word representation},
    booktitle = {In EMNLP},
    year = {2014}
}

@misc{goldberg2015primer,
  abstract = {Over the past few years, neural networks have re-emerged as powerful
machine-learning models, yielding state-of-the-art results in fields such as
image recognition and speech processing. More recently, neural network models
started to be applied also to textual natural language signals, again with very
promising results. This tutorial surveys neural network models from the
perspective of natural language processing research, in an attempt to bring
natural-language researchers up to speed with the neural techniques. The
tutorial covers input encoding for natural language tasks, feed-forward
networks, convolutional networks, recurrent networks and recursive networks, as
well as the computation graph abstraction for automatic gradient computation.},
  added-at = {2017-01-18T15:32:27.000+0100},
  author = {Goldberg, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/29aa674995bbb6c7ef0ff91827a3a6a38/albinzehe},
  description = {[1510.00726] A Primer on Neural Network Models for Natural Language Processing},
  interhash = {0d4411af22df74aa8795081804889c29},
  intrahash = {9aa674995bbb6c7ef0ff91827a3a6a38},
  keywords = {cnn kallimachos neuralnets nlp rnn},
  note = {cite arxiv:1510.00726},
  timestamp = {2017-01-18T15:43:13.000+0100},
  title = {A Primer on Neural Network Models for Natural Language Processing},
  url = {http://arxiv.org/abs/1510.00726},
  year = 2015
}

@article{Bengio:2003:NPL:944919.944966,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 title = {A Neural Probabilistic Language Model},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {1137--1155},
 numpages = {19},
 url = {http://dl.acm.org/citation.cfm?id=944919.944966},
 acmid = {944966},
 publisher = {JMLR.org},
}

@article{wolk_neural-based_2015,
	title = {Neural-based machine translation for medical text domain. {Based} on {European} {Medicines} {Agency} leaflet texts},
	volume = {64},
	issn = {18770509},
	url = {http://arxiv.org/abs/1509.08644},
	doi = {10.1016/j.procs.2015.08.456},
	abstract = {The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neural machine translation. It aims at building a jointly-tuned single neural network that maximizes translation performance, a very different approach from traditional statistical machine translation. Recently proposed neural machine translation models often belong to the encoder-decoder family in which a source sentence is encoded into a fixed length vector that is, in turn, decoded to generate a translation. The present research examines the effects of different training methods on a Polish-English Machine Translation system used for medical data. The European Medicines Agency parallel text corpus was used as the basis for training of neural and statistical network-based translation systems. The main machine translation evaluation metrics have also been used in analysis of the systems. A comparison and implementation of a real-time medical translator is the main focus of our experiments.},
	urldate = {2017-08-08},
	journal = {Procedia Computer Science},
	author = {Wo≈Çk, Krzysztof and Marasek, Krzysztof},
	year = {2015},
	note = {arXiv: 1509.08644},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {2--9}
}

@inproceedings{Koehn:2003:SPT:1073445.1073462,
 author = {Koehn, Philipp and Och, Franz Josef and Marcu, Daniel},
 title = {Statistical Phrase-based Translation},
 booktitle = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1},
 series = {NAACL '03},
 year = {2003},
 location = {Edmonton, Canada},
 pages = {48--54},
 numpages = {7},
 url = {http://dx.doi.org/10.3115/1073445.1073462},
 doi = {10.3115/1073445.1073462},
 acmid = {1073462},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@inproceedings{Koehn:2007:MOS:1557769.1557821,
 author = {Koehn, Philipp and Hoang, Hieu and Birch, Alexandra and Callison-Burch, Chris and Federico, Marcello and Bertoldi, Nicola and Cowan, Brooke and Shen, Wade and Moran, Christine and Zens, Richard and Dyer, Chris and Bojar, Ond\v{r}ej and Constantin, Alexandra and Herbst, Evan},
 title = {Moses: Open Source Toolkit for Statistical Machine Translation},
 booktitle = {Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions},
 series = {ACL '07},
 year = {2007},
 location = {Prague, Czech Republic},
 pages = {177--180},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1557769.1557821},
 acmid = {1557821},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2017-08-10},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@incollection{Rumelhart:1986:LIR:104279.104293,
 author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
 chapter = {Learning Internal Representations by Error Propagation},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {318--362},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=104279.104293},
 acmid = {104293},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{LeCun:1989:BAH:1351079.1351090,
 author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
 title = {Backpropagation Applied to Handwritten Zip Code Recognition},
 journal = {Neural Comput.},
 issue_date = {Winter 1989},
 volume = {1},
 number = {4},
 month = dec,
 year = {1989},
 issn = {0899-7667},
 pages = {541--551},
 numpages = {11},
 url = {http://dx.doi.org/10.1162/neco.1989.1.4.541},
 doi = {10.1162/neco.1989.1.4.541},
 acmid = {1351090},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@article{Hornik:1989:MFN:70405.70408,
 author = {Hornik, K. and Stinchcombe, M. and White, H.},
 title = {Multilayer Feedforward Networks Are Universal Approximators},
 journal = {Neural Netw.},
 issue_date = {1989},
 volume = {2},
 number = {5},
 month = jul,
 year = {1989},
 issn = {0893-6080},
 pages = {359--366},
 numpages = {8},
 url = {http://dx.doi.org/10.1016/0893-6080(89)90020-8},
 doi = {10.1016/0893-6080(89)90020-8},
 acmid = {70408},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
} 