% I may change the way this is done in a future version, 
%  but given that some people needed it, if you need a different degree title 
%  (e.g. Master of Science, Master in Science, Master of Arts, etc)
%  uncomment the following 3 lines and set as appropriate (this *has* to be before \maketitle)
\makeatletter
\renewcommand {\@degree@string} {Master of Science}
\makeatother

\title{Latent Variable Models in Neural Machine Translation}
\author{John Isak Texas Falk}
\department{The Centre for Computational Statistics and Machine Learning}

\maketitle
%\makedeclaration

\leavevmode\thispagestyle{empty}\newpage

% Info in: https://users.ece.cmu.edu/~koopman/essays/abstract.html

\begin{abstract}
  Neural machine translation is the application of neural networks to perform
  translation between two languages, natural language generation concerns itself
  with the generation of sentences using a probabilistic model. Employing latent
  variable models for language generation enables generation of sentences from
  an underlying latent cause. Latent variable models using neural
  networks are very powerful but hard to optimise since the log-likelihood is
  intractable. Variational Autoencoder is a framework for optimising a stochastic approximation of the
  ELBO, a lower bound on the log-likelihood, letting us train these models. While variational autoencoders have been applied with some success to natural language
  processing, it is still unclear if this could be used as a way of translating
  sentence between two languages. We apply the variational autoencoder framework
  to a latent variable model mapping the latent variable to a sentence in two
  distinct languages, showing that after completing training, we can perform
  translation through the use of the recognition model. We evaluate how the
  quality of the generated and translated sentences differ with the power of the
  generative model by qualitative inspection, BLEU, upper bounded perplexity and
  an estimate of the ELBO on a small and big dataset. The results show that
  while the model manages to perform well on shorter sentences it has problems
  generating coherent sentences over longer sequences and that translation is
  possible but unlikely to be competitive with the currently best models.
\end{abstract}

\begin{acknowledgements}
I would like to thank the help from my supervisor Harshil Shah for helping me
make this thesis possible, and my parents, for always being there for me.
\end{acknowledgements}

\setcounter{tocdepth}{2} 
% Setting this higher means you get contents entries for
%  more minor section headers.

\tableofcontents
\listoffigures
\listoftables
