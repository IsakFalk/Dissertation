\message{ !name(Introduction.tex)}
\message{ !name(Introduction.tex) !offset(-2) }
\chapter{Introduction}
\label{IntroductionCh}

% Layout of introduction

%%%
%%% Introduce the field briefly: NLP, VAE, Generative models for language
%%%

Natural language processing (NLP) is an old field with roots in many different
disciplines including but not exclusive to electrical engineering, artificial
intelligence and linguistics \cite[p.~10-15]{Jurafsky:2000:SLP:555733}. Machine
learning often view NLP as a statistical problem. A
language model is a statistical model that trained on a training corpus assign
probabilities to new sentences \cite{Bengio:2003:NPL:944919.944966}. Latent variable models enables these
language models to generate novel sentences from an underlying stochastic
variable. While using probabilistic models enables us to do inference in a
principled manner, it is non-trivial to optimise the log-likelihood
specified by a latent variable model. Variational Inference bounds the log-likelihood
from below using the Evidence Lower Bound (ELBO) by
introducing an approximate distribution, $q(\bm{z})$, of the conditional probability of the
latent variable, $p(\bm{z} | \bm{x})$ \cite{blei_variational_2017}.

%%%
%%% Recent advances: ELBO and relation to SGVB, Relation to NLP, training strong
%%% generative models
%%%

Although Variational Inference specifies ways of approximating the
log-likelihood by a variational distribution $q$ it does not specify the form of
this $q$. Variational Autoencoder (VAE) \cite{kingma_auto-encoding_2013}, also
under the name of stochastic backpropagation \cite{2014arXiv1401.4082J}, is a
framework for training deep generative models with latent variables by
introducing a recognition model $q_{\bm{\varphi}}(\bm{z} | \bm{x})$. Except for the most basic of
models the ELBO function is not tractable. However, it is possibly to
approximate the ELBO by sampling the latent variable instead of
integrating it out. Using the reparameterization trick to sample this latent
variable gives us the differentiable unbiased Stochastic Gradient Variational
Bayes (SGVB) estimator of the ELBO, leading to the Auto-Encoding Variational
Bayes (AEVB) algorithm. AEVB optimises the introduced recognition model
$q_{\bm{\varphi}}(\bm{z} | \bm{x})$ jointly with the generative model
$p_{\bm{\theta}}(\bm{z}, \bm{x})$ with respect to the SGVB objective
\cite{kingma_auto-encoding_2013}. As the recognition model maps the input sentences to the latent space, it effectively tries to compress
information about the sentences through $\bm{z}$, resulting in distributed
latent representations of sentences \cite{bowman_generating_2015}.

%%%
%%% Compare this to other approaches: RNNLM, Context vectors
%%%


%%%
%%% What this thesis tries to do
%%%

%%%
%%% Layout of the thesis
%%%

\message{ !name(Introduction.tex) !offset(-62) }
