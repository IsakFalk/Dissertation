\chapter{Background Knowledge}
\label{BackgroundKnowledgeCh}

From here on I will assume familiarity with some concepts which will be
important for the experiments that we will conduct and analyse.

\section{Probability Theory and Statistics}

\subsection{Probability Theory as a Reasoning System}
Although often seen as a self-contained mathematical theory of stochastic
systems and reasoning about the random, probability theory has a prominent role
within machine learning since it gives us a principled way of reasoning about
the world. As proved by Cox, if we are to accept that any theory of reasoning is
to satisfy the Cox desiderata,
\begin{enumerate}
\item Degrees of plausibility are represented by real numbers.
\item Qualitative correspondence with common sense
\item If a conclusion can be reasoned out in more than one way, then
  every possible way must lead to the same result.
\end{enumerate},
then we must accept that this theory is isomorphic to probability theory and
effectively the same.

This is a formal way of stating that probability theory is the best way to
organize our reasoning if we want to make sure that we are consistent in the way
we reason.\cite[p.~3-23]{jaynes2003probability}

\subsection{Rules and Theorems}
Rules of probability that will be useful to us are the following axioms
\begin{description}
\item[Unit volume]
  \begin{equation}
    \label{eq:unit_vol_prob_axiom}
    \int_{\mathcal{X}}p(X) = 1
  \end{equation}
\item[Non-negativity]
  \begin{equation}
    \label{eq:non_neg_of_prob}
    p(X) \geq 0
  \end{equation}
\end{description}
where $\mathcal{X}$ is the domain of $X$ and the generalized integral is interpreted as
the Lebesgue integral if $X$ is continuous and as a sum over the possible values
of $X$ if it is discrete.

I will assume the notion of a random variable $X$ in an intuitive sense.
However, this $X$ might be represented in many forms and in our case there is
not reason to not be able to put probabilities over sentences, words and/or
characters and conditioning thereof. Similarly I will assume familiarity with
the different notions of continuous, discrete and categorical random variables.
Finally 

Most manipulation of statements about random variables can be stated as a consequence
of the two following fundamental rules
\begin{description}
\item[Sum rule]
  \begin{equation}
    \label{eq:sum_rule}
    p(X) = \int_{\mathcal{Y}}p(X, Y)
  \end{equation}
\item[Product rule]
  \begin{equation}
    \label{eq:product_rule}
    p(X, Y) = p(Y | X)p(X)
  \end{equation}
\end{description}
such that $X, Y$ are two random variables defined on the domains $\mathcal{X},
\mathcal{Y}$. The integral $\int_{\mathcal{Y}}$ is to be understood in the
general sense, if $Y$ is continuous it is the ordinary Lebesgue integral as
commonly used throughout mathematics and calculus, while if $Y$ is discrete then
it is the sum over the possible values of $Y$. $p(X, Y)$ is the joint
distribution of $X$ and $Y$, $p(Y | X)$ is the probability of $Y$ conditioned on
n$X$ and $p(X)$ is the marginal distribution of $X$. Using these rules it is easy
to Bayes theorem, one of the most integral (and simple) theorem of probability
theory

\fbox{
  \begin{minipage}{0.8\textwidth}
    Bayes Theorem

    \begin{equation}
      \label{eq:Bayes_thm}
      p(Y | X) = \frac{p(X | Y)p(Y)}{p(X)}
    \end{equation}
  \end{minipage}
  \hfill
}

Two very important operations involving probabilities of random variables are
those of \textit{Expectation} and \textit{Covariance}. These take as input a
function $f$ and maps to the real number line $\mathbb{R}$, and are defined
implicitly with regards to some random variable $X$ and its 
probability distribution $p(X)$.
\begin{description}
\item[Expectation]
  \begin{equation}
    \label{eq:expectation}
    \E_X[f] = \int_{\mathcal{X}} p(x)f(x)
  \end{equation}
\item[Covariance]
  \begin{equation}
    \label{eq:covariance}
    \Cov(X, Y) = \E_{XY}[(X - \E_X[X])(Y - \E_Y[Y])]
  \end{equation}
\end{description}
We then define the variance operator as
\begin{equation}
  \Var(X) = \Cov(X, X)
\end{equation}

The generalisation from $f: \mathcal{X} \to \mathbb{R}$ to $f: \mathcal{X} \to
\mathbb{R}^n$ is defined in the straightforward manner such that if $\bm{f} =
f(X)$ then
\begin{equation*}
  \E_X
  \begin{bmatrix}
    \bm{f}_1 \\
    \vdots \\
    \bm{f}_n \\
  \end{bmatrix} =
  \begin{bmatrix}
    \E_X \bm{f}_1 \\
    \vdots \\
    \E_X \bm{f}_n \\
  \end{bmatrix}
\end{equation*}
similarly $\Cov(\bm{f})$ is a $D \times D$-dimensional matrix where
$\Cov(\bm{f})_{i,j} = \Cov(\bm{f}_i, \bm{f}_j)$.

\subsection{The Gaussian Distribution}
A very common distribution in machine learning and statistics in general is the
Gaussian distribution, also called the Normal distribution. The Gaussian
distribution satisfies some properties that makes it an ideal candidate from a
modeling perspective including the Central Limit Theorem which says that sums of
independent random variables of finite mean and variance will tend to a normal
distribution, and the fact that the joint distribution of many random variables
that are Gaussianly distributed is itself Gaussian which means that
conditionals, posteriors and other distributions are themselves Gaussian when we
deal with Gaussian random variables.

For a $D$-dimensional vector $\bm{X}$, the multivariate Gaussian distribution
takes the form
\begin{equation}
  \label{eq:Gaussian_dist}
  \mathcal{N}(\bm{x} | \bm{\mu}, \bm{\Sigma}) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\bm{\Sigma}|^{1/2}}\exp\left( -\frac{1}{2}(\bm{x} - \bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu})\right)
\end{equation}
where $\bm{\mu}$ is a $D$-dimensional mean vector, $\bm{\Sigma}$ is a $D \times
D$ dimensional covariance matrix, and $|\bm{\Sigma}|$ denotes the determinant of
$\bm{\Sigma}$. The meaning of these parameters can be shown to correspond to the
operations of the expected value and covariance of $\bm{x}$,
$\E_{\bm{x}}[\bm{x}] = \bm{\mu}$ and $\Cov(\bm{x}) = \bm{\Sigma}$.

The Gaussian distribution can be seen as a unit $D$-dimensional cube which is
translated, sheared and rotated, giving rise to the fact that we can write any
Gaussianly distributed random variable $\bm{x} \sim \mathcal{N}(\bm{x} |
\bm{\mu}, \bm{\Sigma})$ as a linear combination of a unit Gaussian random
variable $\bm{z} \sim \mathcal{N}(\bm{x} | \bm{0}_D, \bm{I}_{D \times D})$. If
we let $\bm{\Lambda} \bm{\Lambda} = \bm{\Sigma}$ be the Cholesky
decomposition\cite[p.~100-102]{Press:2007:NRE:1403886} of $\bm{\Sigma}$, then we
also have that
\begin{equation*}
  p(\bm{x}) = p(\bm{\mu} + \bm{\Lambda}\bm{z})
\end{equation*}
. If we further assume that $\bm{x}$ is parametrised by $\bm{\mu}$ and
$\bm{\Sigma}$ such that $\bm{\Sigma}$ is diagonal positive definite with
diagonal $\bm{\sigma}$, then $\bm{\Sigma} = \bm{\sigma} \odot \bm{I}_{D \times
  D}$. Finally this means that if we want to sample a random variable $\bm{x}$
with diagonal covariance structure, then we can do this by sampling a unit
normal $\bm{z}$ which we then transform, which we can express as
\begin{equation}
  \label{eq:sample_x}
  \bm{x} = \bm{\mu} + \bm{\sigma} \odot \bm{z} \sim \mathcal{N}(\bm{\mu}, \bm{\sigma} \odot \bm{I}_{D \times D})
\end{equation}

As the Gaussian distribution is part of the exponential family, the density of
joint distribution of iid Gaussian variables are themselves Gaussian distributed
where the natural parameters of this joint distribution is the sum of the
natural parameters of each random variable in the joint. In particular for the
Gaussian distribution, this means that if we have a collection of iid gaussian
random variables $\{\bm{x}_i)\}_i^n$, such that $\bm{x}_i \sim
\mathcal{N}(\bm{x}_i | \bm{\mu}_i, \bm{\Sigma}_{i})$, then the joint
can be found to be Gaussian distributed as
\begin{equation}
  \label{eq:join_dist_Gaussian}
  \mathcal{N}(\bm{\mu}, \bm{\Sigma})
\end{equation},
where
\begin{align*}
  \bm{\Sigma} & = \left( \sum_i^n \bm{\Sigma}_i^{-1} \right)^{-1}\\ 
  \bm{\mu} & = \bm{\Sigma}\left( \sum_i^n \bm{\Sigma}^{-1} \bm{\mu}_i \right)
\end{align*}.\cite[p.~78-84]{Bishop:2006}

In the case of two random variables distributed according to the form as laid
out in \ref{eq:sample_x}, $\bm{x} \sim
\mathcal{N}(\mathcal{N}(\bm{\mu}_{\bm{x}}, \bm{\sigma}_{\bm{x}} \odot \bm{I}))$
and $\bm{y} \sim
\mathcal{N}(\mathcal{N}(\bm{\mu}_{\bm{y}}, \bm{\sigma}_{\bm{y}} \odot \bm{I}))$
we have that the resulting distribution $p(\bm{x}, \bm{y}) = p(\bm{x})p(\bm{y})$ is distributed such that
\begin{equation}
  \label{eq:twin_joint_diag_cov}
  p(\bm{x}, \bm{y}) = \mathcal{N}(\bm{\mu}_{\bm{x}, \bm{y}}, \bm{\sigma}_{\bm{x}, \bm{y}})
\end{equation}
where
\begin{align*}
  \bm{\sigma}_{\bm{x}, \bm{y}} & = \frac{1}{\bm{\sigma}_{\bm{x}}^{-1} + \bm{\sigma}_{\bm{y}}^{-1}} \\
  \bm{\mu}_{\bm{x}, \bm{y}} & = \frac{\bm{\sigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}} + \bm{\sigma}_{\bm{y}}^{-1}\bm{\mu}_{\bm{y}}}{\bm{\sigma}_{\bm{x}}^{-1} + \bm{\sigma}_{\bm{y}}^{-1}} \\
\end{align*}.\ref{appendixproofs}

\subsection{Maximum Likelihood Estimation}
An often used estimator for parameters is the \textit{Maximum Likelihood
  Estimator}, hereafter called the MLE.

Assume we have a model $\mathcal{M}$ parametrised by $\bm{\theta}$ constrained
to live in the parameter space $\bm{\Theta}$. Given data $\mathcal{D}$ we want
to be able to fit the parameters $\bm{\theta}$ such that these somehow explains
the data, and thus should be able to work on new data in a manner that
generalises well. The MLE of of the parameters of the model is defined to be
\begin{equation*}
  \hat{\bm{\theta}}_{ML} = \argmax_{\bm{\theta} \in \bm{\Theta}}\mathcal{L}(\bm{\theta}; \mathcal{D})
\end{equation*}
.

While the original MLE is defined in terms of the likelihood function
$\mathcal{L}(\bm{\theta}; \mathcal{D})$, it's often more practical to work with
the logarithm of this function, the log-likelihood function $\ell(\bm{\theta} ;
\mathcal{D})$. Using the common assumption of i.i.d datapoints, the joint
distribution becomes a product of individual probabilities for each datapoint,
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\bm{\theta} | \mathcal{D}) = p(\bm{x}_1, \dots, \bm{x}_n | \bm{\theta}) = \prod_i^n p(\bm{x}_i | \bm{\theta})
\end{equation}.
Using the log-likelihood we transform this product into a form involving sums
\begin{equation}
  \label{eq:log-likelihood}
  \ell(\bm{\theta} | \mathcal{D}) = \log \mathcal{L}(\bm{\theta} | \mathcal{D}) = \sum_i^n \log p(\bm{x}_i)
\end{equation}.
Besides from simplifying notation and calculation, it has the added benefit of
reducing the risk of arithmetic underflow due to the small magnitude of
individual probabilities\footnote{Also, with the use of the log-sum-exp-trick,
  \begin{equation*}
    \log \sum_{i=1}^n \exp(x_n) = \max_i x_i + \log \sum_{i=1}^n \exp(x_n - \max_i x_i)
  \end{equation*}
  this problem can be reduced even further}

In a sense, the MLE is the best fit to the data given that we put a flat prior
over the parameters $\bm{\theta}$, that is we let the data speak for itself. The
MLE only takes into account the information given by the available data
$\mathcal{D}$ and thus makes a choice of $\bm{\theta}$ based upon the
information in the data set and nothing else. Besides this intuitive
justification of the estimator, from a frequentist perspective it comes with a
number of advantageous properties that we like an estimator to have, making it a
natural candidate for training models.

The main problem with MLE's are that they are defined implicitly in terms of the
likelihood function of the data. In that sense every setting is different as
there is no straightforward way to solve for the MLE as we in most cases have to
resort to numerical solutions to try to find the global maximum of
$\mathcal{L}(\bm{\theta}; \mathcal{D})$, a function which is often non-linear,
multimodal and with a complex surface, leading to local maximums being found
instead of the global maximum in many cases.\cite{CaseBerg:01}

\subsection{Graphical models}
A very convenient tool for probabilistic modelling is the notion of using
diagrams to specify the conditional relationships between random variables. A
graphical model is a diagrammatic way of specifying this relationship by
creating a Directed Acyclic Graph, a directed graph without any cycles. This
representation is called a \textit{Graphical Model} and provide a powerful way
to visualize the structure of the probabilistic model and also how to use and
abuse the structure of the model in order to infer variables in a
computationally efficient way.

A graph in this setting consists of a set of \textit{vertices} connected by
\textit{edges}, following the notation and nomenclature of graph theory as used
in mathematics. While there are many different kinds of graphical models
depending on the type of graph structure used (directed graphical models,
undirected graphical models, factor graphs, etc.), we will only focus on the
subset of graphical models called directed graphical models.

Directed graphical models specify how the joint distribution of a set of random
variables $\mathcal{X}$ factors in a conditional manner. In general, the
relationship between a given directed graph and the corresponding distribution
over the variables in $\mathcal{X}$ is such that the join distribution defined
by the graph is given by the product, over all vertices of the graph, of a
conditional distribution for each vertex conditioned on the variables
corresponding to the parents of that vertex in the graph. So for a graph with
$K$ vertices, the joint distribution is give by
\begin{equation}
  \label{eq:dir_graph_model_dist}
  p(\mathcal{X}) = \prod_{k=1}^K p(x_k | \text{pa}(x_k))
\end{equation}
where $\text{pa}(x_k)$ is defined as the set of random variables corresponding
to the parent vertices of the random variable $x_k$.

Although a graphical model is completely defined in terms of it's vertex and
edge set, it is really the most powerful when visualized as a diagram. As an
example I will repeat the above formulation in the context of the directed
graphical model
\begin{figure}[H]
  \center
  \begin{tikzpicture}
    % Define nodes
    \node[latent] (a) {$a$} ;
    \node[latent, right=of a] (b) {$b$} ;
    \node[latent, below=of a, xshift=0.95cm] (c) {$c$} ;
    \node[obs, below=of c] (d) {$d$} ;

    % Connect the nodes
    \edge {a} {b, c} ;
    \edge {b} {c} ;
    \edge {c} {d} ;
  \end{tikzpicture}
\end{figure}
There are two different vertices in this graphical model, the greyed out vertex
indicates that the random variable is \textit{observed} such that it's value is
fixed and known. The white vertices indicates latent random variables which we
don't observe.

For this example we have the following factorisation of the joint distribution,
following the rules laid out in equation~\ref{eq:dir_graph_model_dist},
\begin{equation*}
  p(a, b, c, d) = p(d | c)p(c | a, b)p(b | a)p(a)
\end{equation*}.

\section{Approximate Inference}

While MLE is in many ways the optimal way that we can fit the model, it's only
analytically and/or computationally feasible for very simple models which relies
simple transformations and tractable distributional relationships. In cases
where more powerful models are used it is very hard to find the MLE or even
local maximum to the likelihood $\mathcal{L}$, or equivalently the log-likelihood
$\ell$.

While in theory most of these problems can be resolved by MCMC
sampling, which also practically have been implemented in the way of
probabilistic programming with some
success\cite[Ch.~1]{brooks2011handbook}\cite{Carpenter_stan:a, }

% In a bayesian setting we parametrise a model $\mathcal{M}$ by the parameters
% $\theta_{\mathcal{M}}$ which we will call $\theta$ since our model is fixed. The
% goal is to find the parameters that maximise the likelihood,
% \begin{equation}
%   \label{eq:likelihood}
%   \mathcal{L}(\theta; \mathcal{X}) = p_{\theta}(\mathcal{X})
% \end{equation}.

% Contrary to a normal probability which is a function of the random variable
% $\mathcal{X}$, the likelihood instead acts as a function of $\theta$, hence why
% it's often written in the form $\mathcal{L}(\theta; \mathcal{X})$. However,
% since the likelihood often deals with products due to the iid assumption of the
% data, in practice it is much more common to use the \textit{log-likelihood}
% \begin{equation}
%   \label{eq:log_likelihood}
%   \ell(\theta; \mathcal{X]}) = \log \mathcal{L}(\theta; \mathcal{X})
% \end{equation},
% since this turns the products when assuming iid data into sums
% \begin{equation*}
%   \log p_{\theta}(X_1, \dots, X_n) = \log \prod_{i = 1}^n p_{\theta}(X_i) = \sum_{i = 1}^n \log p_{\theta}(X_i)
% \end{equation*}.

% Beside for ease of notation, this also have the following advantage that when
% dealing with floating point arithmetic, using log-likelihoods we have a much
% smaller chance of over/underflowing. Arithmetic underflow and overflow stems
% from the fact that the computer only deals with floating point numbers, which
% has a smallest number that it can represent, called machine epsilon
% ($\epsilon_{machine}$), and a biggest number that it can represent. When
% multiplying a lot of small numbers together as is common in probability theory,
% the resulting number might be smaller than $\epsilon_{machine}$ which will then
% be rounded down to $0.0$.

% For Maximum Likelihood learning which is the way that we ideally want to train
% models in the

\section{Deep Learning}
A ubiquitous classifier within statistics is logistic regression. Logistic
regression uses an input vector $\bm{x}$ in order to give importance scores in
form of probabilities to different classes $y \in \{c_1, \dots, c_k\}$. It gets
its name from the logistic function
\begin{equation}
  \label{eq:logistic_function}
  \sigma(a) = \frac{1}{1 + e^{a}}
\end{equation}
which together with an affine transformation $\bm{W} \bm{x}$ yields the layer
\begin{equation*}
  \sigma(\bm{W} \bm{x})
\end{equation*}
which transforms values from a feature space $\bm{X} \subset \mathbb{R}^m$ into
probabilities\cite{Bishop:2006}.

Deep learning builds upon this intuition by recursively applying transformations
and activation functions, functions which in some sense maps input on to
\textit{ON/OFF} states. These functions take their functionality from an
abstraction from how neurons function when firing with regards to input,
mirroring how artificial neural networks have taken inspiration from how the
brain operates in the past. On a very basic level, neural networks are
characterised by stacked layers of affine transformations followed by activation
functions, where the output of one layer serves as the input to the next layer.
The final layer outputs $\hat{y}$ where the form of $\hat{y}$ depends on the application.
The hope is that after training the model using
backpropagation\cite{Rumelhart:1995:BBT:201784.201785} that the model is able to
predict satisfactory and drive down the specified loss.

Deep models are very powerful in that they are able to model complex functional
relationships. In our case we are looking at Supervised and Semi-supervised
learning, trying to find the relationship between $\bm{x} \in bm{X}$ and $\bm{y}
\in \bm{Y}$ of some kind of functional form $f(\bm{x}) \approx \bm{y}$.

Besides from the straightforward models where we stack logistic regressors
serially, neural networks have extended well beyond this into an extremely
diverse set of models that can capture different aspects of data such as long
term-dependencies through the architecture of Recurrent Neural Networks and
invariances by using convolutions. Many of these models have also found use in
NLP, especially in the form of RNN's which are well-suited for handling
language due to how it enables information to flow through
time\cite{graves_generating_2013}\cite{cho_learning_2014} and more recently
CNN's for finding representation over many different
scales\cite{semeniuta_hybrid_2017}\cite{yang_improved_2017}\cite{gehring_convolutional_2016}.

In a Bayesian setting each graphical model codifies how different random variables relate
to each other in terms of independency. This is specified by the Directed
Acyclic Graph where each arrow signifies a conditional relationship between
$\bm{x}$ and $\bm{y}$. A full description of how graphical models ,

\section{Natural Language Processing}



