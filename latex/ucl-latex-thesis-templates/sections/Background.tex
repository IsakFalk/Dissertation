\chapter{Background Knowledge}
\label{BackgroundKnowledgeCh}

From here on I will assume familiarity with some concepts which will be
important for the experiments that we will conduct and analyse.

\section{Bayesian Machine Learning}
Bayesian machine learning stems from the tradition of Bayesian statistics where
we assume that the parameters of the model that we are using are random
variables and thus distributed in accordance to some probability distribution.
Letting $\theta$ be the model parameter, this means that before we start doing
anything, we codify our beliefs about $\theta$ in terms of a distribution
$p(\theta)$. Given the model, we then have a way of parametrising the
probability of the data given the model. Assuming that we are doing supervised
learning as in NMT, given a dataset $ \mathcal{D} = \{(x_i, y_i)_i^n\}$ we have a way of
parametrising the functional relationship between $\lang{X}$ and $\lang{Y}$ by
the distribution induced by the model, $p_{\theta}(y_i | x_i)$ representing our
belief about the sentence $y_i$ being the translation given the source
translation $x_i$. Using this we may then find a distribution over the actual
sentences and also do translation by finding the sentences with the highest
probability.

In practice it's not always feasible to put priors on the actual model
parameters $\theta$, instead we see $\theta$ as a parameter that parametrise the
distribution $p_{\theta}(y | x)$.

\section{Deep Learning}
A ubiquitous classifier within statistics is logistic regression. Logistic
regression uses an input vector $\bm{x}$ in order to give importance scores in
form of probabilities to different classes $y \in \{c_1, \dots, c_k\}$. It gets
its name from the logistic function
\begin{equation}
  \label{eq:logistic_function}
  \sigma(a) = \frac{1}{1 + e^{a}}
\end{equation}
which together with an affine transformation $\bm{W} \bm{x}$ yields the layer
\begin{equation*}
  \sigma(\bm{W} \bm{x})
\end{equation*}
which transforms values from a feature space $\bm{X} \subset \mathbb{R}^m$ into
probabilities\cite{Bishop:2006}.

Deep learning builds upon this intuition by recursively applying transformations
and activation functions, functions which in some sense maps input on to
\textit{ON/OFF} states. These functions take their functionality from an
abstraction from how neurons function when firing with regards to input,
mirroring how artificial neural networks have taken inspiration from how the
brain operates in the past. On a very basic level, neural networks are
characterised by stacked layers of affine transformations followed by activation
functions, where the output of one layer serves as the input to the next layer.
The final layer outputs $\hat{y}$ where the form of $\hat{y}$ depends on the application.
The hope is that after training the model using
backpropagation\cite{Rumelhart:1995:BBT:201784.201785} that the model is able to
predict satisfactory and drive down the specified loss.

Deep models are very powerful in that they are able to model complex functional
relationships. In our case we are looking at Supervised and Semi-supervised
learning, trying to find the relationship between $\bm{x} \in bm{X}$ and $\bm{y}
\in \bm{Y}$ of some kind of functional form $f(\bm{x}) \approx \bm{y}$.

Besides from the straightforward models where we stack logistic regressors
serially, neural networks have extended well beyond this into an extremely
diverse set of models that can capture different aspects of data such as long
term-dependencies through the architecture of Recurrent Neural Networks and
invariances by using convolutions. Many of these models have also found use in
NLP, especially in the form of RNN's which are well-suited for handling
language due to how it enables information to flow through
time\cite{graves_generating_2013}\cite{cho_learning_2014} and more recently
CNN's for finding representation over many different
scales\cite{semeniuta_hybrid_2017}\cite{yang_improved_2017}\cite{gehring_convolutional_2016}.

In a Bayesian setting each graphical model codifies how different random variables relate
to each other in terms of independency. This is specified by the Directed
Acyclic Graph where each arrow signifies a conditional relationship between
$\bm{x}$ and $\bm{y}$. A full description of how graphical models ,

\section{Approximate Inference}

In a bayesian setting we parametrise a model $\mathcal{M}$ by the parameters
$\theta_{\mathcal{M}}$ which we will call $\theta$ since our model is fixed. The
goal is to find the parameters that maximise the likelihood,
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\theta; \mathcal{X}) = p_{\theta}(\mathcal{X})
\end{equation}.

Contrary to a normal probability which is a function of the random variable
$\mathcal{X}$, the likelihood instead acts as a function of $\theta$, hence why
it's often written in the form $\mathcal{L}(\theta; \mathcal{X})$. However,
since the likelihood often deals with products due to the iid assumption of the
data, in practice it is much more common to use the \textit{log-likelihood}
\begin{equation}
  \label{eq:log_likelihood}
  \ell(\theta; \matcal{X]}) = \log \mathcal{L}(\theta; \mathcal{X})
\end{equation},
since this turns the products when assuming iid data into sums
\begin{equation*}
  \log p_{\theta}(X_1, \dots, X_n) = \log \prod_{i = 1}^n p_{\theta}(X_i) = \sum_{i = 1}^n \log p_{\theta}(X_i)
\end{equation*}.

Beside for ease of notation, this also have the following advantage that when
dealing with floating point arithmetic, using log-likelihoods we have a much
smaller chance of over/underflowing. Arithmetic underflow and overflow stems
from the fact that the computer only deals with floating point numbers, which
has a smallest number that it can represent, called machine epsilon
($\epsilon_{machine}$), and a biggest number that it can represent. When
multiplying a lot of small numbers together as is common in probability theory,
the resulting number might be smaller than $\epsilon_{machine}$ which will then
be rounded down to $0.0$.

For Maximum Likelihood learning which is the way that we ideally want to train
models in the 


\section{Natural Language Processing}
