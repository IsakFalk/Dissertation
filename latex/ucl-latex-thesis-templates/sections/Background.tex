
\chapter{Background Knowledge}
\label{BackgroundKnowledgeCh}

This chapter will introduce the necessary background knowledge to follow the
theoretical derivations later.

\section{Probability Theory and Statistics}

We go over the rules of probability and statistics.

\subsection{Rules and Theorems}

We here lay out definitions and theorems that we will make use of in the thesis.
For all parts below, assume that $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ are
random variables and $p(x)$ respectively $p(y)$ the probability distributions
of these variables. We interpret the integral over $x$ to be a sum over
$\mathcal{X}$ if $x$ is discrete and a regular Lebesgue integral over
$\mathcal{X}$ if $x$ is continuous. Then:
\begin{definition}[Unit Volume]
  \label{eq:unit_vol_prob_axiom}
  \begin{equation*}
    \int_{\mathcal{X}}p(x) \dif x = 1
  \end{equation*}
\end{definition}
\begin{definition}[Non-negativity]
  \label{eq:non_neg_of_prob}
  \begin{equation*}
    p(x) \geq 0
  \end{equation*}
\end{definition}

Most manipulations of random variables may be reduced to the following three rules of probability:
\begin{theorem}[Sum Rule]
  \label{eq:sum_rule}
  \begin{equation*}
    p(x) = \int_{\mathcal{Y}}p(x, y) \dif y
  \end{equation*}
\end{theorem}
\begin{theorem}[Product Rule]
  \label{eq:product_theorem}
  \begin{equation*}
    p(x, y) = p(y | x)p(x)
  \end{equation*}
\end{theorem}
\begin{theorem}[Bayes Rule]
  \label{eq:Bayes_theorem}
  \begin{equation*}
    p(y | x) = \frac{p(x | y)p(y)}{p(x)}
  \end{equation*}
\end{theorem}

Two very important operations involving probabilities of random variables are
those of \textit{Expectation} and \textit{Covariance}.
\begin{definition}[Expectation]
  \label{eq:expectation}
  Let $f : \mathcal{X} \to \mathbb{R}$, then
  \begin{equation*}
    \E_x[f] = \int_{\mathcal{X}} f(x) p(x) \dif x
  \end{equation*}
\end{definition}
\begin{definition}[Covariance]
  \label{eq:covariance}
  \begin{equation*}
    \Cov(x, y) = \E_{xy}[(x - \E_x[x])(y - \E_y[y])]
  \end{equation*}
\end{definition}
The variance operator is straightforwardly defined:
\begin{definition}[Variance]
  \label{eq:variance}
  \begin{equation*}
    \Var(x) = \Cov(x, x)
  \end{equation*}
\end{definition}

The generalisation of expectation from $f: \mathcal{X} \to \mathbb{R}$ to $f: \mathcal{X} \to
\mathbb{R}^n$ is defined in a direct manner. If $\bm{f} =
f(x)$ then:
\begin{equation*}
  \E_x
  \begin{bmatrix}
    \bm{f}_1 \\
    \vdots \\
    \bm{f}_n \\
  \end{bmatrix} =
  \begin{bmatrix}
    \E_x \bm{f}_1 \\
    \vdots \\
    \E_x \bm{f}_n \\
  \end{bmatrix}
\end{equation*}
similarly $\Cov(\bm{f})$ is a $D \times D$-dimensional matrix where
$\Cov(\bm{f})_{i,j} = \Cov(\bm{f}_i,
\bm{f}_j)$\cite{Bishop:2006, Barber:2012:BRM:2207809}.

\subsection{The Gaussian Distribution}
For a $D$-dimensional random vector $\bm{x}$, the multivariate Gaussian
distribution takes the form
\begin{equation}
  \label{eq:Gaussian_dist}
  \mathcal{N}(\bm{x} | \bm{\mu}, \bm{\Sigma}) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\bm{\Sigma}|^{1/2}}\exp\left( -\frac{1}{2}(\bm{x} - \bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu})\right),
\end{equation}
where $\bm{\mu}$ is a $D$-dimensional mean vector, $\bm{\Sigma}$ is a $D \times
D$ dimensional positive definite covariance matrix and $|\bm{\Sigma}|$ denotes
the determinant of $\bm{\Sigma}$. It is straightforward to show that these
parameters correspond to the mean and covariance as defined in
\ref{eq:expectation} and \ref{eq:covariance}\cite{Barber:2012:BRM:2207809}.

The Gaussian distribution can be seen as a unit $D$-dimensional cube which is
translated, sheared and rotated, giving rise to the fact that we can write any
Gaussianly distributed random variable $\bm{x} \sim \mathcal{N}(\bm{x} |
\bm{\mu}, \bm{\Sigma})$ as a linear combination of a unit Gaussian random
variable $\bm{z} \sim \mathcal{N}(\bm{x} | \bm{0}, \bm{I})$. If
we let $\bm{\Lambda} \bm{\Lambda}^{\top} = \bm{\Sigma}$ be the Cholesky
decomposition\cite[p.~100-102]{Press:2007:NRE:1403886} of $\bm{\Sigma}$, then we
also have that
\begin{equation}
  \label{eq:sample_x}
  \bm{x} = \bm{\mu} + \bm{\Lambda}\bm{z},
\end{equation}
where the equality is in terms of distribution. If we further assume that
$\bm{x}$ is parametrised by $\bm{\mu}$ and $\bm{\Sigma}$ such that $\bm{\Sigma}$
is diagonal positive definite with diagonal $\bm{\sigma}$ then this means that if we want to
sample a random variable $\bm{x}$ with diagonal covariance structure, we
can do this by sampling a unit normal $\bm{z}$ which we then transform, which
may be expressed as
\begin{equation}
  \label{eq:sample_x_diag_covariance}
  \bm{x} = \bm{\mu} + \bm{\sigma} \odot \bm{z} \sim \mathcal{N}(\bm{\mu}, \bm{\sigma}),
\end{equation}
where we define a vector $\bm{\sigma}$ as the covariance matrix $\bm{\Sigma}$ to
mean that $\bm{\Sigma}$ is diagonal positive definite with diagonal $\bm{\sigma}$.

As the Gaussian distribution is part of the exponential
family \cite{Barber:2012:BRM:2207809}, the density of the joint distribution of iid
Gaussian variables are themselves Gaussian distributed where the natural
parameters of this joint distribution is the sum of the natural parameters of
each random variable in the joint. In particular for the Gaussian distribution,
this means that if we have a collection of iid gaussian random variables
$\{\bm{x}_i\}_i^n$, such that $\bm{x}_i \sim \mathcal{N}(\bm{x}_i | \bm{\mu}_i,
\bm{\Sigma}_{i})$, then the joint can be found to be Gaussian distributed as
\begin{equation*}
  \mathcal{N}(\bm{\mu}, \bm{\Sigma})
\end{equation*},
where
\begin{align}
  \bm{\Sigma} & = \left( \sum_i^n \bm{\Sigma}_i^{-1} \right)^{-1} \label{eq:joint_indep_normal_covariance}\\ 
  \bm{\mu} & = \bm{\Sigma}\left( \sum_i^n \bm{\Sigma}^{-1} \bm{\mu}_i \right) \label{eq:joint_indep_normal_mean}
\end{align}\cite[p.~78-84]{Bishop:2006}.

In the case of two independent random variables distributed according to the form as laid
out in \eqref{eq:sample_x_diag_covariance}, $\bm{x} \sim
\mathcal{N}(\bm{\mu}_{\bm{x}}, \bm{\sigma}_{\bm{x}})$
and $\bm{y} \sim \mathcal{N}(\bm{\mu}_{\bm{y}}, \bm{\sigma}_{\bm{y}})$, we have that the resulting distribution $p(\bm{x}, \bm{y}) =
p(\bm{x})p(\bm{y})$ is distributed such that
\begin{equation*}
  p(\bm{x}, \bm{y}) = \mathcal{N}(\bm{\mu}_{\bm{x}, \bm{y}}, \bm{\sigma}_{\bm{x}, \bm{y}})
\end{equation*}
where
\begin{align}
  \bm{\sigma}_{\bm{x}, \bm{y}} & = \frac{1}{\bm{\sigma}_{\bm{x}}^{-1} + \bm{\sigma}_{\bm{y}}^{-1}} \label{eq:joint_indep_normal_covariance_diag}\\
  \bm{\mu}_{\bm{x}, \bm{y}} & = \frac{\bm{\sigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}} + \bm{\sigma}_{\bm{y}}^{-1}\bm{\mu}_{\bm{y}}}{\bm{\sigma}_{\bm{x}}^{-1} + \bm{\sigma}_{\bm{y}}^{-1}} \label{eq:joint_indep_normal_mean_diag}
\end{align}
with the inverse and division operators being done elementwise.

\subsection{Maximum Likelihood Estimation}

Assume we have a model $\mathcal{M}$ parametrised by $\bm{\theta}$ constrained
to live in the parameter space $\bm{\Theta}$. Given data $\mathcal{D} = \{\bm{x}_i\}_{i=1}^n$ we want
to be able to fit the parameters $\bm{\theta}$ such that the model generalise to
unseen data.

We define the likelihood function using the common assumption of i.i.d datapoints
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\bm{\theta} | \mathcal{D}) = p(\bm{x}_1, \dots, \bm{x}_n | \bm{\theta}) = \prod_i^n p(\bm{x}_i | \bm{\theta}).
\end{equation}
The MLE of of the parameters of the model is then defined to be
\begin{equation}
  \label{eq:MLE}
  \hat{\bm{\theta}}_{ML} = \argmax_{\bm{\theta} \in \bm{\Theta}}\mathcal{L}(\bm{\theta} | \mathcal{D}).
\end{equation}

While the original MLE is defined in terms of the likelihood function
$\mathcal{L}(\bm{\theta}| \mathcal{D})$, it's often more practical to work with
the logarithm of this function, the log-likelihood function $\ell(\bm{\theta} |
\mathcal{D})$. Using the log-likelihood we transform this product into a form involving sums
\begin{equation}
  \label{eq:loglikelihood}
  \ell(\bm{\theta} | \mathcal{D}) = \log \mathcal{L}(\bm{\theta} | \mathcal{D}) = \sum_i^n \log p(\bm{x}_i).
\end{equation} \cite{CaseBerg:01}

Besides from simplifying notation and calculation, the log-likelihood has the added benefit of
reducing the risk of arithmetic underflow due to the small magnitude of
individual probabilities. In models with long dependency such as language
models, working in the log-space is needed to enable computation efficiently and
robustly \cite{}.

\subsection{Graphical models}
Graphical models are powerful tools for specifying the dependency relations
between random variables in models. Each graph specifies the class of possible distributions for the model. In particular they provide a clear way
of reasoning about model assumptions and provide a framework for doing inference
and learning \cite{Barber:2012:BRM:2207809}.

Directed graphical models specify how the joint distribution of a set of random
variables $\mathcal{X}$ factors in a conditional manner. In general, the
relationship between a given directed graph and the corresponding distributions
over the variables in $\mathcal{X}$ is such that the join distribution defined
by the graph is given by the product over all vertices of the graph. The
conditional distribution for each vertex is conditioned on the variables
corresponding to the parents of that vertex in the graph. So for a graph with
$K$ vertices, the joint distribution is give by
\begin{equation}
  \label{eq:dir_graph_model_dist}
  p(\mathcal{X}) = \prod_{k=1}^K p(x_k | \text{pa}(x_k))
\end{equation}
where $\text{pa}(x_k)$ is defined as the set of random variables corresponding
to the parent vertices of the random variable $x_k$.

Although a graphical model is completely defined in terms of it's vertex and
edge set, it is really the most powerful when visualized as a diagram. I will
show this with an example using all of the graphical model parts found in the
later section when we specify our models.
\begin{figure}[H]
  \center
  \label{tikz:gm}
  \begin{tikzpicture}
    % Define nodes
    \node[latent] (a) {$a$} ;
    \node[latent, right=of a, xshift=0.8cm] (b) {$b_n$} ;
    \node[latent, below=of b] (c) {$c_n$} ;
    \node[obs, below=of c] (d) {$d_n$} ;

    % Connect the nodes
    \edge {a} {b, c} ;
    \edge {b} {c} ;
    \edge {c} {d} ;

    % Specify plates
    \plate {bcd} {(b)(c)(d)} {$n = 1, \dots, N$} ;
  \end{tikzpicture}
  \caption{Example of a graphical model}
\end{figure}
There are two different vertices in this graphical model, the greyed out vertex
indicates that the random variable is \textit{observed} such that it's value is
fixed and known. The white vertices indicates \textit{latent} random variables which are unobserved.

The edge around some of the vertices specifying that $n = 1, \dots, N$ is an
instance of plate notation. Plate notation signifies that the plate is
duplicated $N$ times and is used for cases where the same subgraph reoccurs many
times as is the case with for example iid data.

For this example we have the following factorisation of the joint distribution,
following the rules laid out in equation~\eqref{eq:dir_graph_model_dist},
\begin{equation*}
  p(a, \{(b_n, c_n, d_n)_{n=1}^N\}) = p(a)\prod_{n=1}^Np(d_n | c_n)p(c_n | b_n, a)p(b_n | a).
\end{equation*}

\section{Deep Learning}
The field of NLP were originally dominated by older machine learning
techniques utilising linear models trained over very high-dimensional and sparse
feature vectors. Recently the field has switched over to neural networks over
dense inputs using word embeddings \cite[p.~1 - 2]{goldberg2015primer}.

Neural networks may be seen from many angles, but from a mathematical point of
view a neural network parametrised by parameters (weights) $\bm{\theta}$
specifies a non-linear functional relationship between the input to the network $\bm{x}$
and the output $\bm{y}$. As such each neural network with unspecified parameters
$\bm{\theta}$ in a parameter space $\Theta$ specifies a set of functions
\cite{Bishop:2006}. It has been shown that this set of functions of several
architectures are universal approximators and thus are able to approximate a big
set of nice functions \cite{Hornik:1989:MFN:70405.70408, Cybenko1989univapprox},
justifying their use theoretically.

\subsection{Multilayer Perceptron}
Multilayer Perceptrons (MLP's) are neural networks represented by functional composition, where each
function is interpreted as a layer of the network. The original MLP can be defined in
terms of a recurrence relation such that if we have input vectors of the form $\bm{x} \in
\mathbb{R}^{d_{in}}$ and output vectors of the form $\bm{y} \in
\mathbb{R}^{d_{out}}$ and $\sigma_i( \cdot )$ represents an arbitrary activation
function, then an MLP with $L$ layers have the functional form
\begin{equation}
  f(\bm{x} | \bm{\theta}) = \sigma_L(\bm{W}_L \bm{z}_{L-1} + \bm{b}_{L})
\end{equation}
where for any $l \in \{2, \dots, L-1\}$
\begin{equation}
    \bm{z}_l = \sigma_l(\bm{W}_l \bm{z}_{l-1} + \bm{b}_l)
\end{equation}
with base case
\begin{equation}
  \bm{z}_1 = \sigma_1(\bm{W}_1 \bm{x} + \bm{b}_1).
\end{equation} \cite{BarberAppliedML}

$\bm{W}_l$ and $\bm{b}_l$ may be of any dimension as long as it is dimensionally consistent
with the input and output of the previous and next layers and conform to the
original input and output dimensions. In this case we have that the parameters
of the network are all of the biases and weights for the layers, $\bm{\theta} =
\{(\bm{W}_l, \bm{b}_l)_{l = 1}^L\}$.

Activation functions generally are monotonically increasing function mapping real
numbers to some interval. Activation functions which will be useful to us are
\begin{description}
\item[Sigmoid]
  \begin{equation}
    \label{eq:sigmoid}
    \sigma(x) = \frac{1}{1 + \exp(-x)}
  \end{equation}
  The sigmoid activation function maps any real number to a value in $(0, 1)$.
\item[Hyperbolic tangent]
  \begin{equation}
    \label{eq:tanh}
    \tanh(x) = \frac{2}{1 + \exp(-2x)} - 1 = 2 \sigma(2x) - 1
  \end{equation}
\item[SELU]
  Scaled Exponential Linear Units \cite{DBLP:journals/corr/KlambauerUMH17} have the form
  \begin{equation}
    \label{eq:SELU}
    \text{selu}(x) = \lambda
    \begin{cases}
      x \quad\text{if $x > 0$} \\
      \alpha(\exp(x) - 1) \quad \text{if $x \leq 0$}
    \end{cases}
  \end{equation}
\item[Softmax]
  The Softmax is a mapping from an input space $\mathbb{R}^{d_{in}}$ to an output
  space $\mathbb{R}^{d_{out}}$, the form given by
  \begin{equation}
    \label{eq:softmax}
    \text{softmax}(\bm{z})_i = \frac{\exp(z_i)}{\sum_{k=1}^{d_{out}}\exp(z_k)} \quad \text{where} \quad i \in \{1, \dots, d_{out}\}
  \end{equation}
  Since it is normalized it is a natural candidate for defining categorical
  distributions \cite{Barber:2012:BRM:2207809} and we will use it for this
  purpose in our models.
\end{description}

\subsection{Recurrent Neural Networks}

RNN's have traditionally been the neural network architecture of choice for NLP
due to it being able to handle long-term dependencies well \cite{KarpathyRNN}.



For in depth treatment of the architecture with respect to nlp see \cite{Hochreiter:1997:LSM:1246443.1246450, sundermeyer2012lstm}.

\subsection{Convolutional Neural Networks}
\label{ch:WaveNet}
The CNN we will use will be highly specialised for our case. We use the WaveNet
as laid out in the paper by DeepMind \cite{DBLP:journals/corr/OordDZSVGKSK16}.

WaveNet works on data of the form of vectors, $\bm{x}$, were we describe the distribution as
\begin{equation*}
p(\bm{x}) = \prod_{l=1}^L p(\bm{x}_l| \bm{x}_1, \dots, \bm{x}_{l-1}),
\end{equation*}
the form common in NLP and NMT. The output of the model has the same
time dimensionality as the input, meaning that the model has the ability to
output a categorical distribution over the next value $x_l$ with a softmax
layer. In our case this is a parametrisation of the output probability at point
$l$.

WaveNet uses dilated convolutions, a type of convolution where the filter is applied
to an area larger than its length by only considering input values such that at
each step you skip $k$ inputs, where $k$ is the dilation. Stacking layers and
using dilations that increase exponentially with the depth of the layer, it is
possible to get a receptive field which grows exponentially with the depth of
the network \cite{DBLP:journals/corr/OordDZSVGKSK16}.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{wavenet_dilated_convolutions}
  \caption{Dilated Convolutions in WaveNet \cite{DBLP:journals/corr/OordDZSVGKSK16}}
  \label{fig:wavenet_dilated_convolutions}
\end{figure}

\section{Natural Language Processing}

Humans use natural language every day to convey concepts and abstractions to each
other in an efficient manner. Compared to formal languages found in
mathematics and programming, the natural languages we use are often
ambiguous systems filled with rules and exceptions \cite{Rosenfeld00twodecades, sep-computational-linguistics}.

Natural Language Processing is an old field that for a long
time developed in parallel with the field of machine learning and
computational statistics which deals with how to process information coming from
human languages and is split up into several subfields such as Machine
Translation, statistical parsing and sentiment
analysis \cite{sep-computational-linguistics}.

We will here lay out the necessities for understanding the neural language model we
will build.

\subsection{Language model}
We define a sentence to be a vector of words $\bm{x}_{1:L} = (\mathsf{x}_1, \dots,
\mathsf{x}_L)^{\top}$ such that each word is an atomic element $\mathsf{x}_i \in
V$, where $V$ is the dictionary of words in our language. Repeated use of the
rule laid out in \ref{eq:product_theorem} enables us to rewrite the probability
of a sentence in terms of how it is composed of words
\begin{equation}
  \label{eq:conditional_language_probability}
  P(\bm{x}_{1:L}) = \prod_{l = 1}^LP(\mathsf{x}_l | \mathsf{x}_1, \dots, \mathsf{x}_{l-1})
\end{equation}
where $\mathsf{x}_l$ is the $l$'th word of the sentence
$\bm{x}_{1:L}$ \cite{Bengio:2003:NPL:944919.944966}.

This form of the probability of a sentence lays bare how the words in a sentence
relate to previously occurred words within the same sentence. We will assume
that all sentences in the dataset $\mathcal{D} = \{(\bm{x}^n)_{n=1}^N\}$ are iid
which enables us to express the log-likelihood in the form of equation
\eqref{eq:loglikelihood}.

\subsection{Word embeddings}
Breaking down sentences at a word level and processing them into a form that
encodes information efficiently is a problem which has gained notorious
recognition \cite{DBLP:journals/corr/abs-1301-3781}, leading to algorithms such
as word2vec \cite{Mikolov:2013:DRW:2999792.2999959} and
Glove \cite{Pennington14glove:global}. However, these techniques work less well
in a neural network setting where instead the preferred technique is to find the
best embeddings jointly with the parameters of the model using backpropagation \cite[p.~5-7]{goldberg2015primer}.

A straightforward way to represent the various words of the dictionary is as
one-hot-encoded vectors such that a word $\mathsf{x} \in V$, where the size of
$V$ is $|V|$, with an index $i$ given by its place in the dictionary sorted
alphabetically in descending order will have the vector representation
\begin{equation}
  \label{eq:one_hot_encoding}
  \bm{x}_{one-hot} =
  \begin{bmatrix}
    0 \\
    \vdots \\
    0 \\
    1 \\
    0 \\
    \vdots \\
    0
  \end{bmatrix}
\end{equation}
such that $\bm{x}_{one-hot, j} = \delta_{ij}$ \cite[p.~6]{goldberg2015primer}.

While this is a form conceptually easy to understand, it fails to account for the
curse of dimensionality as the size of the vocabulary grows very big in practice
and the fact that the cosine similarity of two words $\mathsf{x}_1,
\mathsf{x}_2 \in V$ is zero unless they are the same word,
\begin{equation}
  \label{eq:cosine_similarity}
  \cos_{similarity}(\bm{x}_{1, one-hot}, \bm{x}_{2, one-hot}) = \delta_{\mathsf{x}_1 \mathsf{x}_2}.
\end{equation} This means that no meaning is embedded in the vector space
except for the location in the sorted dictionary. Instead we would like to
associate each word in the vocabulary with a distributed \textit{word feature
  vector}, a dense, real-valued vector in $\mathbb{R}^m$ where $m$ is the
dimension of our embedding space. After training this embedding jointly with the
parameters of the network words which share similarities such as \texttt{Dog,
  Puppy} would have a higher similarity score than with an unrelated concepts such as
\texttt{Dog, Bulwark} \cite{Bengio:2003:NPL:944919.944966}.

We may represent this in a mathematical form by trying to find a linear map $C$
from any element $\mathsf{x} \in V$ such that $C(\mathbf{x}) \in \mathbb{R}^m$.
Using the canonical basis of the Euclidean space, we can express this
linear map in terms of a matrix $\bm{C} \in \mathbb{R}^{m \times |V|}$, thus the
word feature vector of the learned embedding can be represented by the matrix
multiplication $\bm{C}\bm{x}_{one-hot}$.

\subsection{Neural machine translation}
For a long time the dominant paradigm within machine translation was to use
phrase based machine translation systems \cite{Koehn:2003:SPT:1073445.1073462,
  Koehn:2007:MOS:1557769.1557821}, recently however, modelling the word or
character level directly with neural networks has overtaken phrase based
translation systems \cite{wolk_neural-based_2015, wu_googles_2016}. These
systems are called Neural Machine Translation.

Most NMT models work in terms of an encoder-decoder architecture where the
encoder extracts a fixed length representation $\bm{c}$, often called a context
vector, from a variable length input sentence $\bm{x} \in \lang{X}$, and the
decoder uses this representation to generate a correct translation $\bm{y} \in
\lang{Y}$ from this representation \cite{cho_properties_2014} as can be seen in
Figure ~\ref{fig:encoder_decoder}.

\begin{figure}[H]
  \includestandalone[width=\textwidth]{./scripts/tikz_code/encoder_decoder}% 
    \caption{Encoder decoder schematic}
  \label{fig:encoder_decoder}
\end{figure}

Our model builds on this paradigm but instead of considering context vectors
$\bm{c}$ we instead consider distributions over a latent vector $\bm{z}$ which
in theory should give the model a greater expressive power since the
distribution relays more knowledge than a direct point estimate.

\section{Optimization}

Most parts of machine learning reduces to optimization of a loss function. While
optimisation of convex problems are well-understood there has been considerable
interest in ways to optimize non-linear loss functions such as those used
traditionally in deep learning.

While theoretical results are lacking, there has been substantial
advances in various optimisation techniques fit to attack the highly non-linear,
non-convex and high-dimensional optimization problems of learning in deep
models, particularly from Stochastic Gradient Descent and its friends. This has
led to numerous gradient descent-like algorithms used machine learning
\cite{Ruder17}.

\subsection{Stochastic Gradient Descent}

For a normal probabilistic machine learning problem, we have data $\mathcal{D}$
that we try to model with a model $\mathcal{M}$ parametrised by parameters
$\bm{\theta} \in \Theta$. The optimization problem in our case can be recast
as an effort to find the parameters $\bm{\theta}_{ML}$ that maximizes the
log-likelihood function \eqref{eq:loglikelihood} or equally minimizes the
negative log-likelihood.

Gradient descent takes steps in the direction of the gradient, which also is the
direction of steepest descent. If we let $\mathcal{D} = \{\bm{x}_i\}_{i = 1}^n$
be our set of sentences and $-\ell$ be the negative log-likelihood that we are
trying to minimize then the updates are of the form
\begin{equation}
  \label{eq:GD_update}
  \bm{\theta}_{t + 1} = \bm{\theta}_t - \gamma \frac{1}{n} \sum_{i = 1}^n \nabla_{\bm{\theta}} (-\ell(\bm{\theta}| \bm{x}_i)),
\end{equation}
where $\gamma$ is the learning rate of the algorithm.

Stochastic Gradient Descent is a similar algorithm to GD that instead of
calculating the gradient with respect to the whole dataset calculates an
approximate gradient, as it only takes a subset of the data into account at each
update. If we let $I_t$ be a random subset of the indices of $\{(\bm{x}_n)_{n=1}^N\}$ of size $m$ then SGD does the following update
\begin{equation}
  \label{eq:SGD_update}
  \bm{\theta}_{t + 1} = \bm{\theta}_t - \gamma \frac{1}{m} \sum_{i \in I_t} \nabla_{\bm{\theta}} (-\ell(\bm{\theta}| \bm{x}_i))
\end{equation}\cite{series/lncs/Bottou12}\cite[p.~240]{Bishop:2006}.

\subsection{ADAM}
SGD has found widespread use within the machine learning community due to strong
experimental results and ease of use, especially in deep learning. However there
are notable alternatives that try to improve on SGD such as
RMSProp\cite{Tieleman2012} and AdaGrad\cite{Duchi:EECS-2010-24}.

Adam takes inspiration from RMSProp and AdaGrad. Technically, Adam keeps an
exponential running average of the first and second order statistics of the
gradient, using these to calculate an adaptive learning rate.

As laid out in the original paper by Kingma et al. \cite{kingma_adam:_2014}, ADAM operates on a
stochastic objective function $f_t(\bm{\theta})$, where $t$ denotes the time
step, similarly $g_t = \nabla_{\bm{\theta}}f_t(\bm{\theta})$. $m_t$ and $v_t$ denotes the first and second order moments of
the gradients, however, these are biased and are corrected in the algorithm.
The whole algorithm is as follows.
\begin{algorithm}[H]
  \caption{ADAM}\label{ADAM}
  \begin{algorithmic}[1]
    \Require $\alpha$: Stepsize
    \Require $\beta_1, \beta_2 \in [0, 1)$: Exponential decay rates of the moment estimates
    \Require $f(\bm{\theta})$: Stochastic objective function with parameters
    $\bm{\theta}$
    \Require $\bm{\theta}_0$: Initial parameter vector
    \State $m_0 \gets 0$ (Initialize $1^{\text{st}}$ moment vector)
    \State $v_0 \gets 0$ (Initialize $2^{\text{nd}}$ moment vector)
    \State $t \gets 0$ (Initialize timestep)
    \While{$\bm{\theta}_t$ not converged}
    \State $t \gets t + 1$
    \State $g_t \gets \nabla_{\bm{\theta}}f_t(\bm{\theta}_{t-1})$ (get gradients w.r.t stochastic objective at timestep $t$)
    \State $m_t \gets \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$ (Update biased first moment estimate)
    \State $v_t \gets \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g^2_t$ (Update biased second raw moment estimate)
    \State $\hat{m}_1 \gets m_t / (1 - \beta^t_1)$ (Compute bias-corrected first moment estimate)
    \State $\hat{v}_1 \gets v_t / (1 - \beta^t_2)$ (Compute bias-corrected second raw moment estimate)
    \State $\bm{\theta}_t \gets \bm{\theta}_{t-1} - \alpha \cdot \hat{m}_i/(\sqrt{\hat{v}_t} + \epsilon)$ (Update parameters)
    \EndWhile
    \Return $\bm{\theta}_t$ (Resulting parameters)
  \end{algorithmic}
  \caption{The ADAM algorithm}
\end{algorithm}

Experimentally Adam has shown very good results on training various deep
learning models such as MLP's, CNN's and RNN's so we will use it to train our
models throughout this dissertation \cite{kingma_adam:_2014}.

\subsection{Automatic Differentiation}
\label{ch:autodiff}
The AutoDiff framework is a general theory for efficiently
calculating gradients with respect to a loss function in general computational graphs.
It includes the famed backpropagation \cite{Bishop:2006,
  Rumelhart:1986:LIR:104279.104293} algorithm as a special instance.

In machine learning we most often deal with many-to-one mappings with a
high-dimensional input $\bm{x}$ to a scalar output in the form of a loss. In
terms of machine learning models, AutoDiff takes a loss function
$L(\bm{\theta})$ and returns an exact value up to machine accuracy for the
gradient $\nabla_{\bm{\theta}} L(\bm{\theta}) \vert_{\bm{\theta}}$. There are
two different modes of AutoDiff, forward and reverse, although in practice
reverse is preferred.

Reverse mode AutoDiff is efficient in the way that calculating the gradient of
$L(\bm{\theta})$ is guaranteed to take at most 5 times the time it takes to
compute $L(\bm{\theta})$ \cite{Barber15deeplearning:}, enabling neural network
libraries to calculate the gradients needed for optimization algorithms such as
SGD and ADAM in a swift and automatic manner.
