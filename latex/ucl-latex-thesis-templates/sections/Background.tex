\chapter{Background Knowledge}
\label{BackgroundKnowledgeCh}

From here on I will assume familiarity with some concepts which will be
important for the experiments that we will conduct and analyse.

\section{Probability Theory and Statistics}

\subsection{Probability Theory as a Reasoning System}
Although often seen as a self-contained mathematical theory of stochastic
systems and reasoning about the random, probability theory has a prominent role
within machine learning since it gives us a principled way of reasoning about
the world. As proved by Cox, if we are to accept that any theory of reasoning is
to satisfy the Cox desiderata,
\begin{enumerate}
\item Degrees of plausibility are represented by real numbers.
\item Qualitative correspondence with common sense
\item If a conclusion can be reasoned out in more than one way, then
  every possible way must lead to the same result.
\end{enumerate},
then we must accept that this theory is isomorphic to probability theory and
effectively the same.

This is a formal way of stating that probability theory is the best way to
organize our reasoning if we want to make sure that we are consistent in the way
we reason.\cite[p.~3-23]{jaynes2003probability}

\subsection{Rules and Theorems}
Rules of probability that will be useful to us are the following axioms
\begin{description}
\item[Unit volume]
  \begin{equation}
    \label{eq:unit_vol_prob_axiom}
    \int_{\mathcal{X}}p(X) = 1
  \end{equation}
\item[Non-negativity]
  \begin{equation}
    \label{eq:non_neg_of_prob}
    p(X) \geq 0
  \end{equation}
\end{description}
where $\mathcal{X}$ is the domain of $X$ and the generalized integral is interpreted as
the Lebesgue integral if $X$ is continuous and as a sum over the possible values
of $X$ if it is discrete.

I will assume the notion of a random variable $X$ in an intuitive sense.
However, this $X$ might be represented in many forms and in our case there is
not reason to not be able to put probabilities over sentences, words and/or
characters and conditioning thereof. Similarly I will assume familiarity with
the different notions of continuous, discrete and categorical random variables.
Finally 

Most manipulation of statements about random variables can be stated as a consequence
of the two following fundamental rules
\begin{description}
\item[Sum rule]
  \begin{equation}
    \label{eq:sum_rule}
    p(X) = \int_{\mathcal{Y}}p(X, Y)
  \end{equation}
\item[Product rule]
  \begin{equation}
    \label{eq:product_rule}
    p(X, Y) = p(Y | X)p(X)
  \end{equation}
\end{description}
such that $X, Y$ are two random variables defined on the domains $\mathcal{X},
\mathcal{Y}$. The integral $\int_{\mathcal{Y}}$ is to be understood in the
general sense, if $Y$ is continuous it is the ordinary Lebesgue integral as
commonly used throughout mathematics and calculus, while if $Y$ is discrete then
it is the sum over the possible values of $Y$. $p(X, Y)$ is the joint
distribution of $X$ and $Y$, $p(Y | X)$ is the probability of $Y$ conditioned on
$X$ and $p(X)$ is the marginal distribution of $X$. Using these rules it is easy
to Bayes theorem, one of the most integral (and simple) theorem of probability
theory

\fbox{
  \begin{minipage}{0.8\textwidth}
    Bayes Theorem

    \begin{equation}
      \label{eq:Bayes_thm}
      p(Y | X) = \frac{p(X | Y)p(Y)}{p(X)}
    \end{equation}
  \end{minipage}
  \hfill
}

Two very important operations involving probabilities of random variables are
those of \textit{Expectation} and \textit{Covariance}. These take as input a
function $f$ and maps to the real number line $\mathbb{R}$, and are defined
implicitly with regards to some random variable $X$ and its 
probability distribution $p(X)$.
\begin{description}
\item[Expectation]
  \begin{equation}
    \label{eq:expectation}
    \expectation{f, X} = \int p(x)f(x)
  \end{equation}
\item[Covariance]
  \begin{equation}
    \label{eq:covariance}
    \cov{X, Y} = \expectation{(X - \expectation{X, X})(Y - \expectation{Y, Y}), XY}
  \end{equation}
\end{description}
We then define the variance operator as
\begin{equation}
  \var{X} = \cov{X, X}
\end{equation}

\subsection{The Gaussian Distribution}


\subsection{Maximum Likelihood Estimation}
The methods that we will use in our experiments stem from the tradition of
\textit{Bayesian Statistics}. In short, in Bayesian statistics we assume that
the parameters we are trying to find are not fixed as is common to assume when
thinking of probability statements as long-term frequencies of events. Instead
we assume that probabilities are statements about our \textit{beliefs} of the
parameter or random variable we are interested in.

In our case, we won't use the full Bayesian machinery, we will instead be
interested in doing \textit{Maximum Likelihood} optimization which means that if
we let $\mathcal{D} = \{(x_i, y_i)_i^n\}$ be our dataset and $\theta$ be all the
parameters of our model of the distribution of the data and $\Theta$ be the
domain of the model parameters, then maximum likelihood estimation in the case
of supervised learning is formulated as the following problem of finding the
model parameters
\begin{equation}
  \label{eq:ML_estimate}
  \theta_{ML} = \argmax_{\theta \in \Theta}p(y_1, \dots, y_n | x_1, \dots, x_n)
\end{equation}

In some way this is the optimal setting of $\theta$ without prior knowledge of
how they are distributed. It takes all the data available into account and
reduces to finding the $\theta$ that maximizes the \textit{likelihood}, that is
the statement $p_{\theta}(X)$ seen as a function in $\theta$.

\section{Deep Learning}
A ubiquitous classifier within statistics is logistic regression. Logistic
regression uses an input vector $\bm{x}$ in order to give importance scores in
form of probabilities to different classes $y \in \{c_1, \dots, c_k\}$. It gets
its name from the logistic function
\begin{equation}
  \label{eq:logistic_function}
  \sigma(a) = \frac{1}{1 + e^{a}}
\end{equation}
which together with an affine transformation $\bm{W} \bm{x}$ yields the layer
\begin{equation*}
  \sigma(\bm{W} \bm{x})
\end{equation*}
which transforms values from a feature space $\bm{X} \subset \mathbb{R}^m$ into
probabilities\cite{Bishop:2006}.

Deep learning builds upon this intuition by recursively applying transformations
and activation functions, functions which in some sense maps input on to
\textit{ON/OFF} states. These functions take their functionality from an
abstraction from how neurons function when firing with regards to input,
mirroring how artificial neural networks have taken inspiration from how the
brain operates in the past. On a very basic level, neural networks are
characterised by stacked layers of affine transformations followed by activation
functions, where the output of one layer serves as the input to the next layer.
The final layer outputs $\hat{y}$ where the form of $\hat{y}$ depends on the application.
The hope is that after training the model using
backpropagation\cite{Rumelhart:1995:BBT:201784.201785} that the model is able to
predict satisfactory and drive down the specified loss.

Deep models are very powerful in that they are able to model complex functional
relationships. In our case we are looking at Supervised and Semi-supervised
learning, trying to find the relationship between $\bm{x} \in bm{X}$ and $\bm{y}
\in \bm{Y}$ of some kind of functional form $f(\bm{x}) \approx \bm{y}$.

Besides from the straightforward models where we stack logistic regressors
serially, neural networks have extended well beyond this into an extremely
diverse set of models that can capture different aspects of data such as long
term-dependencies through the architecture of Recurrent Neural Networks and
invariances by using convolutions. Many of these models have also found use in
NLP, especially in the form of RNN's which are well-suited for handling
language due to how it enables information to flow through
time\cite{graves_generating_2013}\cite{cho_learning_2014} and more recently
CNN's for finding representation over many different
scales\cite{semeniuta_hybrid_2017}\cite{yang_improved_2017}\cite{gehring_convolutional_2016}.

In a Bayesian setting each graphical model codifies how different random variables relate
to each other in terms of independency. This is specified by the Directed
Acyclic Graph where each arrow signifies a conditional relationship between
$\bm{x}$ and $\bm{y}$. A full description of how graphical models ,

\section{Approximate Inference}

In a bayesian setting we parametrise a model $\mathcal{M}$ by the parameters
$\theta_{\mathcal{M}}$ which we will call $\theta$ since our model is fixed. The
goal is to find the parameters that maximise the likelihood,
\begin{equation}
  \label{eq:likelihood}
  \mathcal{L}(\theta; \mathcal{X}) = p_{\theta}(\mathcal{X})
\end{equation}.

Contrary to a normal probability which is a function of the random variable
$\mathcal{X}$, the likelihood instead acts as a function of $\theta$, hence why
it's often written in the form $\mathcal{L}(\theta; \mathcal{X})$. However,
since the likelihood often deals with products due to the iid assumption of the
data, in practice it is much more common to use the \textit{log-likelihood}
\begin{equation}
  \label{eq:log_likelihood}
  \ell(\theta; \mathcal{X]}) = \log \mathcal{L}(\theta; \mathcal{X})
\end{equation},
since this turns the products when assuming iid data into sums
\begin{equation*}
  \log p_{\theta}(X_1, \dots, X_n) = \log \prod_{i = 1}^n p_{\theta}(X_i) = \sum_{i = 1}^n \log p_{\theta}(X_i)
\end{equation*}.

Beside for ease of notation, this also have the following advantage that when
dealing with floating point arithmetic, using log-likelihoods we have a much
smaller chance of over/underflowing. Arithmetic underflow and overflow stems
from the fact that the computer only deals with floating point numbers, which
has a smallest number that it can represent, called machine epsilon
($\epsilon_{machine}$), and a biggest number that it can represent. When
multiplying a lot of small numbers together as is common in probability theory,
the resulting number might be smaller than $\epsilon_{machine}$ which will then
be rounded down to $0.0$.

For Maximum Likelihood learning which is the way that we ideally want to train
models in the 


\section{Natural Language Processing}
