\chapter{Introduction}
\label{IntroductionCh}

% Layout of introduction

%%%
%%% Introduce the field briefly: NLP, VAE, Generative models for language
%%%

Natural language processing (NLP) is an old field with roots in many different
disciplines including but not exclusive to electrical engineering, artificial
intelligence and linguistics \cite[p.~10-15]{Jurafsky:2000:SLP:555733}. Machine
learning often view language as a statistical problem. A
language model is a statistical model that trained on a training corpus assign
probabilities to new sentences. Using latent variable models enables these
language models to generate novel sentences from an underlying stochastic
variable. While using probabilistic models enables us to do inference in a
principled manner, it is often non-trivial to find the optimal parameters of
latent variable models. Variational Inference approximates the log-likelihood
through the Evidence Lower Bound (ELBO) which bound it from below, by
introducing an approximate distribution of the conditional probability of the
latent variable, $p(\bm{z} | \bm{x})$ \cite{blei_variational_2017}.

%%%
%%% Recent advances: ELBO and relation to SGVB, Relation to NLP, training strong
%%% generative models
%%%

Although Variational Inference specifies ways of approximating the
log-likelihood by a variational distribution $q$ it does not specify the form of
this $q$. Variational Autoencoder (VAE) \cite{kingma_auto-encoding_2013}, also
under the name of stochastic backpropagation \cite{2014arXiv1401.4082J}, is a
framework for training deep generative models with latent variables by
introducing a recognition model $q_{\bm{\phi}}$. Except for the most basic of
models the ELBO function is not tractable, however, using
the reparametrisation trick and the law of large numbers, it is possibly to
approximate the ELBO by sampling the latent variable instead of
integrating it out, giving us the Stochastic Gradient Variational Bayes (SGVB)
algoritm. The recognition model may then be optimized jointly with the
generative model $p_{\bm{\theta}}$ with respect to the SGVB objective \cite{kingma_auto-encoding_2013}. As the recognition model maps the
input sentences to the latent space, it effectively tries to compress
information about the sentences through $\bm{z}$, resulting in distributed
latent representations of sentences \cite{bowman_generating_2015}. After
learning the recognition mapping from $\bm{x}, \bm{y}$ to $\bm{z}$ we may use
this form to do translation by omitting one of the input languages, effectively
letting a source language decide the distribution over the latent space, which
can be mapped back through the generative model to both $\lang{X}$ and
$\lang{Y}$ performing both reconstruction and translation concurrently.

%%%
%%% Compare this to other approaches: RNNLM, Context vectors
%%%


%%%
%%% What this thesis tries to do
%%%

%%%
%%% Layout of the thesis
%%%
