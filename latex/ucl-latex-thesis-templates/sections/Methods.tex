\chapter{Methods and Theory}
\label{MethodsCh}

This chapter deals with the theory we will be using in order to use VAE in the
our setting. It will also deal with extending the theory and necessary calculations.

\subsection{Variational Inference}
Consider the following general graphical model of a latent variable $\bm{z}$ and
an observed variable $\bm{x}$:
\begin{figure}[H]
  \center
  \label{tikz:latent_variable_model}
  \begin{tikzpicture}
    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z] (x) {$\bm{x}$} ;
    %\node[const, right=of z, xshift=-0.5] (theta) {$\bm{\theta}$} ;

    % Connect the nodes
    \edge {z} {x} ;
    %\edge {theta} {z, x} ;

    % Plate
    % \plate {zx} {(z)(x)} {$n$} ;
  \end{tikzpicture}
  \caption{Latent variable model}
\end{figure}
implying the joint distribution
\begin{equation}
  \label{eq:latent_variable_model}
  p(\bm{x}, \bm{z}) = p(\bm{x} | \bm{z}) p(\bm{z})
\end{equation}

In order to fit the parameters of the model using MLE we need to be able to to
find the gradient and value of $\ell(\bm{\theta})$ to use ADAM. We can rewrite
the likelihood for a datapoint $\bm{x}$ as
\begin{equation}
  \label{eq:likelihood_integrate_out_latent}
  \mathcal{L}(\bm{\theta} ; \bm{x}) = \int_{\mathcal{X}} p(\bm{x}, \bm{z}) \dif X
\end{equation}
by using the sum rule \ref{eq:sum_rule}. For many models, this integral is
either not available in a closed form or requires exponential time to compute
\cite{blei_variational_2017} and while for some special cases it is possible to
find a local optimum of the likelihood by using the EM algorithm
\cite{Dempster77maximumlikelihood} in general it is too restrictive since we
need to be able to find $p(\bm{z} | \bm{x})$ analytically.

While it is possible to use sampling based techniques such as MCMC
\cite{brooks2011handbook} to sample from $p(\bm{z} | \bm{x})$ in practice it is
often slow and depending on the problem may be less than ideal. Especially for
problems where we use complex models and have large datasets it's not possible
in practice to use MCMC.

Variational inference approaches the problem of optimizing the intractable
log-likelihood by introducing a variational distribution $q$ belonging to some
constrained set of predefined distributions $\mathcal{Q}$. Since the
log-likelihood is intractable, we try to bound the log-likelihood from below by
an expression that is easier to deal with. Using Jensen's inequality with the
$\log( \cdot )$ function, we have that

\begin{align*}
  \log p_{\bm{\theta}}(\bm{x}) & = \log \int_{\mathcal{Z}} p_{\bm{\theta}}(\bm{z}, \bm{x}) \dif \bm{z}\\
                               & = \log \int_{\mathcal{Z}}q_{\bm{\varphi}}(\bm{z})  \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})} \dif \bm{z} \\
                               & = \log \E_{q_{\bm{\varphi}}(\bm{z})}\left[\frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})}\right] \\
                               & \geq \E_{q_{\bm{\varphi}}(\bm{z})}\left[\log \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z})}\left[\log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log q_{\bm{\varphi}}(\bm{z})\right] \\
\end{align*}

Defining the term Evidence Lower Bound Objective,
\begin{equation}
  \label{eq:ELBO}
  \text{ELBO}(q_{\bm{\varphi}}) =
  \E_{q_{\bm{\varphi}}(\bm{z})}\left[ \log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log
    q_{\bm{\varphi}}(\bm{z}) \right]  
\end{equation}
we express this as the following resulting inequality
\begin{equation}
  \label{eq:ELBO_inequality}
  \log p_{\bm{\theta}}(\bm{x}) \geq \text{ELBO}(q_{\bm{\varphi}})
\end{equation}.
However, we note that we can rewrite the ELBO as
\begin{equation}
  \label{eq:ELBO_decomposed}
  \text{ELBO}(q_{\bm{\varphi}})) = \log p_{\bm{\theta}}(\bm{x}) - \KL{q_{\bm{\varphi}}}{}
\end{equation}

\section{ELBO}

Ideally we would like to optimize the log-likelihood of the data,
$\ell(\bm{\theta} ; \mathcal{D})$. For many models, it is not possible to
evaluate this quantity directly due to it being computationally and/or
analytically intractable.

Instead we make the following observation, letting $\mathcal{D} = \{\bm{x}_i\}_{i=1}^N$
\begin{equation}
  \label{eq:log_likelihood_sum}
  \log p_{\bm{\theta}}(\mathcal{D}) = \sum_{i=1}^N \log p_{\bm{\theta}}(\bm{x}_i)
\end{equation}
and looking at each term in the sum we can see that we may rewrite this as
\begin{align*}
  \log p_{\bm{\theta}}(\bm{x}_i) & = \log \int_{\mathcal{Z}} p_{\bm{\theta}}(\bm{z}, \bm{x}_i) \dif \bm{z}\\
                               & = \log \int_{\mathcal{Z}}q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)  \frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)} \dif \bm{z} \\
                               & = \log \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\right] \\
                               & \geq \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log \frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log \frac{p_{\bm{\theta}}(\bm{x}_i | z) p_{\bm{\theta}}(\bm{z})}{q_{\bm{\varphi}(\bm{z} | \bm{x}_i)}}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log p_{\bm{\theta}}(\bm{x}_i | \bm{z})] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}{p_{\bm{\theta}}(\bm{z})}\right]
\end{align*}
Which leads to the following observation, if we call the lower bound
$\mathcal{L}(\bm{\theta}, \bm{\varphi}; \bm{x})$, that
\begin{equation}
  \label{eq:ELBO_inequality}
  \log p_{\bm{\theta}}(\bm{x}) \geq \mathcal{L}(\bm{\theta}, \bm{\varphi}; \bm{x}) = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}.

In general this lower bound is still not tractable, however appealing to the
Strong Law of Large Numbers (SLLN) we can sample $\bm{z}^{l}$ and use the
reparametrisation trick (Include how the reparametrisation trick works) to approximate the lower bound by
\begin{equation}
  \label{eq:SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = \frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log q_{\bm{\varphi}}(\bm{z} | \bm{x})
\end{equation}.
This can also be expressed in the closed form of the KL so
\begin{equation}
  \label{eq:SGVB_analytical_KL}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = (\frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x} | \bm{z})) - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}

Since all quantities in this form have analytically known forms, it is simple to
evaluate and sample from $\tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x})$.

\section{Models}

The generative model follows the one laid out in
\cite{kingma_auto-encoding_2013}:

\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z] (x) {$\bm{x}$} ;
    \node[const, right=of z] (thetax){$\bm{\theta}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {thetax} {x} ;

    % Make plate
    \plate {zx} {(z)(x)} {$N$};
    
  \end{tikzpicture}
\end{figure}

where $\bm{z}$ is the latent variable representing the latent sentence structure
and the joint factorizes as $p_{\bm{\theta}}(\bm{x}, \bm{z}) =
p_{\bm{\theta}}(\bm{x} | \bm{z})p(\bm{z})$. In all models we let $p(\bm{z}) =
\mathcal{N}(\bm{z}| \bm{0}, \bm{I}_{D \times D})$.

\subsection{Reconstruction}

The reconstruction model follows the original model. We assume a latent variable
$\bm{z}$ that generates sentences $\bm{x}$ belonging to a language $\lang{X}$.

The ELBO and related calculations follow the ones laid out in the ELBO section.

\subsection{Translation}

Translation adds and extra observed variable $\bm{y}$, a sentence belonging to
$\lang{Y}$. The probabilistic model looks like:

\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z, xshift=-1.0cm] (x) {$\bm{x}$} ;
    \node[obs, below=of z, xshift=1.0cm] (y) {$\bm{y}$} ;
    \node[const, left=of z, xshift=-0.5cm] (thetax) {$\bm{\theta}_{\bm{x}}$} ;
    \node[const, right=of z, xshift=0.5cm] (thetay) {$\bm{\theta}_{\bm{y}}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {z} {y} ;
    \edge {thetax} {x} ;
    \edge {thetay} {y} ;

    % Make plate
    \plate {zxy} {(z)(x)(y)} {$N$} ;
    
  \end{tikzpicture}
\end{figure}

From the graphical model we see that the joint distribution factorises such that
\begin{equation}
  \label{eq:joint_generative}
  p(\bm{\bm{z}}, \bm{x}, \bm{y}) = p(\bm{z})p(\bm{x} | \bm{z})p(\bm{y} | \bm{z}) = \mathcal{N}(0, I)\mathcal{N}(\mu_{\bm{\theta}_{\bm{x}}}(\bm{z}), \sigma_{\bm{\theta}_{\bm{x}}}(\bm{z}))\mathcal{N}(\mu_{\bm{\theta}_{\bm{y}}}(\bm{z}), \sigma_{\bm{\theta}_{\bm{y}}}(\bm{z}))
\end{equation}.
The notation $\mathcal{N}(\mu_{\bm{\theta}}(\bm{z}), \sigma_{\bm{\theta}}(\bm{z}))$
means that the mean and variance diagonal of the normal are represented by a
functional relationship $f: \bm{z} \mapsto (\mu, \sigma)$ where $f$ is a function
represented a a learnable neural network with parameters $\bm{\theta}$.

The recognition model is parametrised by the probability distribution
$q_{\varphi}(z | x, y)$. We are free to choose this however we like, but the closer
this is to the actual conditional distribution over the latent, $p(z | x, y)$
the tighter the ELBO inequality $\ell(x, y) \geq \mathcal{L}(x, y)$ will be. We
let the distributions all be gaussians to have analytical tractability
\begin{align*}
  q(z | x, y) & = q(z | x)q(z | y) \\
              & = N(\mu(x), \sigma(x) I) N(\mu(y), \sigma(y) I)
\end{align*}
where both normals defined by the output of neural networks.

Since the product of Guassian pdf's are gaussian itself, we have that may write
that
\begin{equation*}
  q(\bm{z} | \bm{x}, \bm{y}) = \mathcal{N}(\bm{z} | \mu(\bm{x}, \bm{y}), \sigma(\bm{x}, \bm{y}))
\end{equation*} which reduces to the expressions
\ref{eq:joint_indep_normal_mean} for the mean and
\ref{eq:joint_indep_normal_covariance} for the covariance.
Pulling all of these statements together, we get that the SGVB of the
translation case reduces to the equation
\begin{equation}
  \begin{split}
  \label{eq:translation_SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = (\frac{1}{L} \sum_{l=1}^L \log(\mathcal{N}(\bm{x} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z}))) + \log(\mathcal{N}(\bm{y} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z})))) + \\ \frac{1}{2} \sum_{j=1}^J(1 + \log((\sigma_j^{(i)})^2) - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2))
  \end{split}
\end{equation}

where the $\mu, \sigma$ is from the normal $\mathcal{N}(\bm{z} | \mu(\bm{x},
\bm{y}), \sigma(\bm{x}, \bm{y}))$.

\subsection{Training}
Variational Autoencoders in NLP have a tendency to collapse the KL term in the SGVB
objective to zero, effectively being reduced a RNNLM (CITE PAPER). As the KL can
be seen as a measure of how much information is encoded in $\bm{z}$ this event
makes the use of VAE's to model language-agnostic features in the latent space
useless. One of the objectives is to make sure this KL-collapse doesn't happen.

Following the tips laid out in (CITE PAPER VAE FOR NLP) we use KL-annealing
where we anneal the ELBO by a pseudo-objective
\begin{equation}
  \label{eq:KL_annealing_ELBO}
  ELBO_{\beta_t} = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \beta_t * \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}
where $\beta_t$ is an annealing term going from $0$ to $1$ as we train. We do
this by choosing a warm-up $\beta$ and let $\beta_t = \min(1, \frac{t}{\beta})$,
letting this warm up be done linearly until we recover the original ELBO
objective, similar to normal simulated annealing (CITE ANNEALING PAPER).

