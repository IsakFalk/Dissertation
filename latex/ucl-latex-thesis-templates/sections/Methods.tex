\chapter{Methods and Theory}
\label{MethodsCh}

\section{ELBO}

Ideally we would like to optimize the log-likelihood of the data,
$\ell(\bm{\theta} ; \bm{\theta})$. For many models, it is not possible to
evaluate this quantity directly due to it being computationally and/or
analytically intractable.

Instead we make the following observation
\begin{equation}
  \label{eq:log_likelihood_sum}
  \log p_{\bm{\theta}}(\mathcal{D}) = \sum_{i=1}^N \log p_{\bm{\theta}}(\bm{x}_i)
\end{equation}
and looking at each term in the sum we can see that we may rewrite this as (Let
$\bm{x}$ be known from context)
\begin{align*}
  \log p_{\bm{\theta}}(\bm{x}) & = \log \int_{\mathcal{Z}} p_{\bm{\theta}}(\bm{z}, \bm{x})\\
                               & = \log \int_{\mathcal{Z}}q_{\bm{\phi}}(\bm{z} | \bm{x}) \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\phi}}(\bm{z} | \bm{x})} \\
                               & = \log \E_{q_{\bm{\phi}}(\bm{z} | \bm{x})[\frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\phi}}(\bm{z} | \bm{x})}}] \\
                               & \geq \E_{q_{\bm{\phi}}(\bm{z} | \bm{x})}[\log \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\phi}}(\bm{z} | \bm{x})}] \\
                               & = \E_{q_{\bm{\phi}}(\bm{z} | \bm{x})}[\frac{p_{\bm{\theta}}(\bm{x} | z) p_{\bm{\theta}}(\bm{z})}{q_{\bm{\phi}(\bm{z} | \bm{x})}}] \\
                               & = \E_{q_{\bm{\phi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \KL{q_{\bm{\phi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{align*}
Which leads to the following observation, if we call the lower bound
$\mathcal{L}(\bm{\theta}, \bm{\phi}; \bm{x})$, that
\begin{equation}
  \label{eq:ELBO_inequality}
  \log p_{\bm{\theta}}(\bm{x}) \geq \mathcal{L}(\bm{\theta}, \bm{\phi}; \bm{x}) = \E_{q_{\bm{\phi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \KL{q_{\bm{\phi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}.

In general this lower bound is still not tractable, however appealing to the LLN
we can sample $\bm{z}^{l}$ and use the reparametrisation trick to approximate
the lower bound by
\begin{equation}
  \label{eq:SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\phi}; \bm{x}) = \frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log q_{\bm{\phi}}(\bm{z} | \bm{x})
\end{equation}
However, if we use good priors and likelihoods, like categorical and/or normals,
we can evaluate the KL analytically and we don't need to sample, giving use the
form
\begin{equation}
  \label{eq:SGVB_analytical_KL}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\phi}; \bm{x}) = (\frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x} | \bm{z})) - \KL{q_{\bm{\phi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}

\subsection{Translation case}

In our case we have due to the graphical model

\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z, xshift=-1.0cm] (x) {$\bm{x}$} ;
    \node[obs, below=of z, xshift=1.0cm] (y) {$\bm{y}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {z} {y} ;
    
  \end{tikzpicture}
\end{figure}

where $\bm{x}$ is a sentence in language $\lang{X}$ and $\bm{y}$ is a sentence in language $\lang{Y}$.
From the graphical model we see that the joint distribution factorises such that
\begin{equation}
  \label{eq:joint_generative}
  p(z, x, y) = p(z)p(x | z)p(y | z) = N(0, I)N(\mu_x(z), \sigma_x(z))N(\mu_y(z), \sigma_y(z))
\end{equation}
where the normal quantities are parametrised by neural networks.

The recognition model is parametrised by the probability distribution
$q_{\phi}(z | x, y)$. We are free to choose this however we like, but the closer
this is to the actual conditional distribution over the latent, $p(z | x, y)$
the tighter the ELBO inequality $\ell(x, y) \geq \mathcal{L}(x, y)$ will be. We
let the distribution all be gaussians to have analytical tractability
\begin{align*}
  q(z | x, y) & = q(z | x)q(z | y) \\
              & = N(\mu(x), \sigma(x) I) N(\mu(y), \sigma(y) I)
\end{align*}
where both normals defined by the output of neural networks.

Since the product of Guassian pdf's are gaussian itself, we have that may write
that
\begin{equation*}
  q(\bm{z} | \bm{x}, \bm{y}) = \mathcal{N}(\bm{z} | \mu(\bm{x}, \bm{y}), \sigma(\bm{x}, \bm{y}))
\end{equation*}.
Pulling all of these statements together, we get that the SGVB of the
translation case reduces to the equation
\begin{equation}
  \label{eq:translation_SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\phi}; \bm{x}) = (\frac{1}{L} \sum_{l=1}^L \log(\mathcal{N}(\bm{x} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z}))) + \log(\mathcal{N}(\bm{y} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z})))) + \frac{1}{2} \sum_{j=1}^J(1 + \log((\sigma_j^{(i)})^2) - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2))
\end{equation}

where the $\mu, \sigma$ is from the normal $\mathcal{N}(\bm{z} | \mu(\bm{x}, \bm{y}), \sigma(\bm{x}, \bm{y}))$.
