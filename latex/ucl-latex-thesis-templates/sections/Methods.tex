\chapter{Methods and Theory}
\label{MethodsCh}

This chapter deals with the theory needed in order to use VAE in our setting. It
will also deal with extending the theory and necessary calculations.

\section{Variational Inference}
Consider the following general graphical model of a latent variable $\bm{z}$ and
an observed variable $\bm{x}$ parametrised by $\bm{\theta}$:
\begin{figure}[H]
  \label{fig:latent_variable_model}
  \centering
  \begin{tikzpicture}
    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z] (x) {$\bm{x}$} ;
    \node[const, right=of z, xshift=-0.5] (theta) {$\bm{\theta}$} ;

    % Connect the nodes
    \edge {z} {x} ;
    \edge {theta} {z, x} ;

    % Plate
    \plate {zx} {(z)(x)} {$N$} ;
  \end{tikzpicture}
  \caption{Graphical model of a latent variable model parametrised by $\bm{\theta}$.}
\end{figure}
implying the joint distribution
\begin{equation}
  \label{eq:latent_variable_model}
  p(\bm{x}, \bm{z}) = p(\bm{x} | \bm{z}) p(\bm{z}).
\end{equation}

In order to train the model using optimisation of
$\ell(\bm{\theta} | \mathcal{D})$ we need to be able to to find the gradient and
value of $\ell(\bm{\theta} | \bm{x})$ which are used by ADAM. We can rewrite
the likelihood for a datapoint $\bm{x}$ as
\begin{equation}
  \label{eq:likelihood_integrate_out_latent}
  \mathcal{L}(\bm{\theta} | \bm{x}) = \int_{\mathcal{Z}} p(\bm{x}, \bm{z}) \dif \bm{z}
\end{equation}
by using the sum rule of \ref{eq:sum_rule}. For many models, this integral is
either not available in a closed form or requires exponential time to compute
\cite{blei_variational_2017}. While for some special cases it is possible to
find a local optimum of the likelihood by using the EM algorithm
\cite{Dempster77maximumlikelihood}, in general it is too restrictive since we
need to be able to find $p(\bm{z} | \bm{x})$ analytically.

Generally it is possible to use sampling based techniques such as MCMC
\cite{brooks2011handbook} to sample from $p(\bm{z} | \bm{x})$ but in practice it is
often slow and depending on the problem may be less than ideal. Especially for
problems where we use complex models and have large datasets it's not possible
in practice to use MCMC \cite{blei_variational_2017}.

Variational inference approaches the problem of optimizing the intractable
log-likelihood by positing a variational family of distributions,
$\mathcal{Q}$ and introducing a variational distribution $q \in \mathcal{Q}$. The
log-likelihood is then bounded from below using the concavity of the
$\log( \cdot )$ function in order to apply Jensen's inequality and the fact that
$q$ is a distribution:
\begin{align*}
  \log p_{\bm{\theta}}(\bm{x}) & = \log \int_{\mathcal{Z}} p_{\bm{\theta}}(\bm{z}, \bm{x}) \dif \bm{z}\\
                               & = \log \int_{\mathcal{Z}}q_{\bm{\varphi}}(\bm{z})  \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})} \dif \bm{z} \\
                               & = \log \E_{q_{\bm{\varphi}}(\bm{z})}\left[\frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})}\right] \\
                               & \geq \E_{q_{\bm{\varphi}}(\bm{z})}\left[\log \frac{p_{\bm{\theta}}(\bm{x}, \bm{z})}{q_{\bm{\varphi}}(\bm{z})}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z})}\left[\log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log q_{\bm{\varphi}}(\bm{z})\right].
\end{align*}

We call the lower bound the Evidence Lower Bound Objective (ELBO),
\begin{equation}
  \label{eq:ELBO}
  \text{ELBO}(q_{\bm{\varphi}}(\bm{z})) =
  \E_{q_{\bm{\varphi}}(\bm{z})}\left[ \log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log
    q_{\bm{\varphi}}(\bm{z}) \right]
\end{equation}
and we get the following inequality
\begin{equation}
  \label{eq:ELBO_inequality}
  \log p_{\bm{\theta}}(\bm{x}) \geq \text{ELBO}(q_{\bm{\varphi}}(\bm{z})).
\end{equation}

Rewriting the ELBO in terms of the log-likelihood
\begin{equation}
  \label{eq:ELBO_decomposed}
  \text{ELBO}(q_{\bm{\varphi}}(\bm{z})) = \log p_{\bm{\theta}}(\bm{x}) - \KL{q_{\bm{\varphi}}(\bm{z})}{p_{\bm{\theta}}(\bm{z} | \bm{x})}
\end{equation}
we see how the choice of $q_{\bm{\varphi}}(\bm{z})$ affects the tightness of the
lower bound through the closeness to the true posterior $p_{\bm{\theta}}(\bm{z}
| \bm{x})$ in terms of the KL-divergence \cite{blei_variational_2017}.

\section{VAE and SGVB}

The Variational Autoencoder by Kingma et al. \cite{kingma_auto-encoding_2013}
introduces a recognition model $q_{\bm{\varphi}}(\bm{z} | \bm{x})$ to a latent
variable model of the form \eqref{eq:latent_variable_model}, that serves as
an approximation to the true posterior $p_{\bm{\theta}}( \bm{z} | \bm{x})$. We let $q_{\bm{\varphi}}(\bm{z} | \bm{x}) \sim
\mathcal{N}(\bm{z}| \bm{\mu_{\bm{\varphi}}}(\bm{x}),
\bm{\sigma}^2_{\bm{\varphi}}(\bm{x}))$ such that the pair of vectors
$(\bm{\mu}_{\bm{\varphi}}(\bm{x}), \bm{\sigma}^2_{\bm{\varphi}}(\bm{x}))$ is the
output of a neural network, and let this Gaussian have a diagonal covariance
structure with the vector $\bm{\sigma}^2_{\bm{\varphi}}(\bm{x})$ as the diagonal.

In the following we assume that $\bm{x}$ is an observed datapoint from our
dataset and $\bm{z}$ is the corresponding latent variable. If we consider the
ELBO again we can rewrite it in terms that only involve known functions and quantities
\begin{equation}
  \label{eq:ELBO_VAE}
  \text{ELBO}(q_{\bm{\varphi}}(\bm{z} | \bm{x})) = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}\left[ \log p_{\bm{\theta}}(\bm{x} | \bm{z}) \right] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}.
\end{equation}
Since this depends on both the parameters of the generative model and the
recognition model, $(\bm{\theta}, \bm{\varphi})$ we make this explicit and write
\begin{equation}
  \label{eq:VAE_ELBO}
  \mathcal{L}^{\text{ELBO}}(\bm{\theta}, \bm{\varphi} | \bm{x}) = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}\left[ \log p_{\bm{\theta}}(\bm{x} | \bm{z}) \right] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}.
\end{equation}

Note that for most models we are not able to get an analytical form of the
expectation over $q_{\bm{\varphi}}(\bm{z} | \bm{x})$ due to the non-linear
relationship between $\bm{x}$ and $\bm{z}$. Instead we sample $\bm{z}$ to
estimate this expectation using normal Monte Carlo estimation.

While it would be possible to differentiate and optimize the lower bound
$\mathcal{L}^{\text{ELBO}}(\bm{\theta}, \bm{\varphi} | \bm{x})$ jointly with
respect to both the variational and generative parameters $\bm{\varphi}$ and
$\bm{\theta}$ by using a straightforward Monte Carlo gradient estimator with respect
to $q_{\bm{\varphi}}(\bm{z} | \bm{x})$, this gradient exhibits very high
variance and is impractical compared to the reparametrisation trick which we will introduce below \cite{kingma_auto-encoding_2013}.

\subsection{Reparametrisation Trick}
Under certain mild regularity conditions, for a chosen approximate posterior
$q_{\bm{\varphi}}(\bm{z} | \bm{x})$ we can express the distribution of $\bm{z}
\sim q_{\bm{\varphi}}(\bm{z} | \bm{x})$ in terms of a simpler distribution using
reparametrisation, called the reparametrisation trick
\cite{kingma_auto-encoding_2013}. For our case, let transformation
$g_{\bm{\varphi}}(\bm{\epsilon}, \bm{x})$ be such that
\begin{equation}
  \label{eq:reparametrisation_trick}
  \bm{z} = g_{\bm{\varphi}}(\bm{\epsilon}, \bm{x}), \quad \text{where} \quad \bm{\epsilon} \sim \mathcal{N}(\bm{0}, \bm{I}).
\end{equation}
In our case we use \eqref{eq:sample_x_diag_covariance} in order to write
$g_{\bm{\varphi}}(\bm{\epsilon}, \bm{x}) = \bm{\mu}_{\bm{\varphi}}(\bm{x}) +
\bm{\sigma}_{\bm{\varphi}}(\bm{x}) \odot \bm{\epsilon}$.

This means that we can form Monte Carlo estimates of the ELBO by sampling
$\bm{z}$ through sampling $\bm{\epsilon}$ and using the differentiable
transformation
\begin{equation}
  \label{eq:SGVB}
    \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})} \left[ \log p_{\bm{\theta}}(\bm{x},  \bm{z}) - \log q_{\bm{\varphi}}(\bm{z} | \bm{x}) \right] \simeq \frac{1}{S} \sum_{s=1}^S \log p_{\bm{\theta}}(\bm{x},  \bm{z}^{s}) - \log q_{\bm{\varphi}}(\bm{z}^{s} | \bm{x})
\end{equation}
where $\bm{z}^{s} = g_{\bm{\varphi}}(\bm{\epsilon}^{s}, \bm{x})$ and
$\bm{\epsilon}^{s} \sim \mathcal{N}(\bm{0}, \bm{I})$ for all $s \in \{1, \dots, S\}$.

The right hand side of equation \eqref{eq:SGVB} is the Stochastic Gradient
Variational Bayes estimator of the ELBO, which we call
$\mathcal{L}^{SGVB}(\bm{\theta}, \bm{\varphi}| \bm{x})$ and is an unbiased
estimator of the ELBO since we sample $\bm{z}$ from the recognition model.

\subsection{AEVB algorithm}
The AEVB algorithm optimises the SGVB equation \eqref{eq:SGVB} in order to in
drive up the ELBO and push the lower bound of the the log-likelihood up. If the
recognition model and the prior are chosen to be from appropriate distributions
it is possible to get an analytical form of the KL-divergence. Choosing
$p_{\bm{\theta}}(\bm{z})$ and $q_{\bm{\varphi}}(\bm{z} | \bm{x})$ to be
Gaussianly distributed enables us to write the SGVB in an alternative form which
typically has less variance than the SGVB form of equation \eqref{eq:SGVB}:
\begin{equation}
  \label{eq:SGVB_analytical_KL}
  \mathcal{L}^{SGVB}(\bm{\theta}, \bm{\varphi}) = -\KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})} + \frac{1}{S} \sum_{s=1}^S \log p_{\bm{\theta}}(\bm{x} | \bm{z}^{s})
\end{equation}
where $\bm{z}^{s} = g_{\bm{\varphi}}(\bm{\epsilon}^{s}, \bm{x})$ and
$\bm{\epsilon}^{s} \sim \mathcal{N}(\bm{0},
\bm{I})$. We let $S = 1$ following the advice
of Kingma et al. \cite{kingma_auto-encoding_2013}.

\section{Models}

Here we lay out the graphical models we will use in order to model language
generation. All of our models are latent variable models with the prior on
$\bm{z}$ being unit normal, $p(\bm{z}) = \mathcal{N}(\bm{z}| \bm{0}, \bm{I})$.

\subsection{Mono-Language Model}

The generative model is the same as in the figure for latent variable models
% \ref{tikz:latent_variable_model}
, except that the distribution of $\bm{z}$ does not depend on $\bm{\theta}$:
\begin{figure}[H]
  \center
  \label{tikz:reconstruction_model}
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z] (x) {$\bm{x}$} ;
    \node[const, right=of z] (thetax){$\bm{\theta}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {thetax} {x} ;

    % Make plate
    \plate {zx} {(z)(x)} {$N$};
    
  \end{tikzpicture}
  \caption{Graphical representation of the Mono-language model. The prior over
    the latent is independent of $\bm{\theta}$.}
\end{figure}
where $\bm{z}$ is the latent variable representing the latent sentence structure
and the joint factorizes as $p_{\bm{\theta}}(\bm{x}, \bm{z}) =
p_{\bm{\theta}}(\bm{x} | \bm{z})p(\bm{z})$.

The Mono-language model is the same model specified in
\cite{kingma_auto-encoding_2013}. Bowman et al. has successfully showed that it
can be applied to natural language, learning meaningful representations of
language in the latent dimension \cite{bowman_generating_2015} while improving
on previous models such as RNNLM \cite{conf/icassp/MikolovKBCK11}.

\subsection{Twin-Language Model}

Using two languages adds and extra observed variable $\bm{y}$, a sentence belonging to
$\lang{Y}$. The probabilistic model looks like:
\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z, xshift=-1.0cm] (x) {$\bm{x}$} ;
    \node[obs, below=of z, xshift=1.0cm] (y) {$\bm{y}$} ;
    \node[const, left=of z, xshift=-0.5cm] (thetax) {$\bm{\theta}_{\bm{x}}$} ;
    \node[const, right=of z, xshift=0.5cm] (thetay) {$\bm{\theta}_{\bm{y}}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {z} {y} ;
    \edge {thetax} {x} ;
    \edge {thetay} {y} ;

    % Make plate
    \plate {zxy} {(z)(x)(y)} {$N$} ;
    
  \end{tikzpicture}
  \caption{Twin-language generative latent variable model. The prior over
    the latent is independent of $\bm{\theta}_{\bm{x}}$ and $\bm{\theta}_{\bm{x}}$.}
\end{figure}

The graphical model implies that the joint distribution factorises as
\begin{equation}
  \label{eq:joint_generative}
  p_{\bm{\theta}}(\bm{\bm{z}}, \bm{x}, \bm{y}) = p(\bm{z})p_{\bm{\theta}_{\bm{x}}}(\bm{x} | \bm{z})p_{\bm{\theta}_{\bm{y}}}(\bm{y} | \bm{z})
\end{equation}
and we make the following distributional assumption
\begin{align}
  p_{\bm{\theta}_{\bm{x}}}(\bm{x} | \bm{z}) & = \mathcal{N}(\bm{\mu}_{\bm{\theta}_{\bm{x}}}(\bm{z}), \bm{\sigma}^2_{\bm{\theta}_{\bm{x}}}(\bm{z})) \\
  p_{\bm{\theta}_{\bm{y}}}(\bm{y} | \bm{z}) & = \mathcal{N}(\bm{\mu}_{\bm{\theta}_{\bm{y}}}(\bm{z}), \bm{\sigma}^2_{\bm{\theta}_{\bm{y}}}(\bm{z}))
\end{align}

The notation $\mathcal{N}(\bm{\mu}_{\bm{\theta}}(\bm{z}), \bm{\sigma}^2_{\bm{\theta}}(\bm{z}))$
means that the mean and variance diagonal of the normal are represented by a
functional relationship $f: \bm{z} \mapsto (\bm{\mu}, \bm{\sigma}^2)$ where $f$ is a function
represented a a learnable neural network with parameters $\bm{\theta}$.

The recognition model parametrises the probability distribution
$q_{\bm{\varphi}}(\bm{z} | \bm{x}, \bm{y})$. We are free to choose this
parametrisation and distributional form however we like, but the closer this is to the actual conditional distribution over the
latent, $p(\bm{z} | \bm{x}, \bm{y})$ in the KL sense, the tighter the
inequality in \eqref{eq:ELBO_inequality} will be as shown by equation
\eqref{eq:ELBO_decomposed}. We let the distributions be Gaussians to have
analytical tractability, 
\begin{align*}
  q_{\bm{\varphi}}(\bm{z} | \bm{x}, \bm{y}) & = q_{\bm{\varphi}_{\bm{x}}}(\bm{z} | \bm{x})q_{\bm{\varphi}_{\bm{y}}}(\bm{z} | \bm{y}) \\
                                            & = \mathcal{N}(\bm{\mu}_{\bm{\varphi}_{\bm{x}}}(\bm{x}), \bm{\sigma}^2_{\bm{\varphi}_{\bm{x}}}(\bm{x}))\mathcal{N}(\bm{\mu}_{\bm{\varphi}_{\bm{y}}}(\bm{y}), \bm{\sigma}^2_{\bm{\varphi}_{\bm{y}}}(\bm{y})).
\end{align*}

This is again a Gaussian distribution
\begin{equation}
  \label{eq:joint_variational_gaussian}
  q_{\bm{\varphi}}(\bm{z} | \bm{x}, \bm{y}) = \mathcal{N}(\bm{z} | \mu_{\bm{\varphi}}(\bm{x}, \bm{y}), \sigma^2_{\bm{\varphi}}(\bm{x}, \bm{y}))
\end{equation} which reduces to the expressions
\eqref{eq:joint_indep_normal_mean} for the mean and
\eqref{eq:joint_indep_normal_covariance} for the covariance as shown in the
background section on the Gaussian distribution.
Pulling all of these statements together, we get that the SGVB of the
translation case reduces to the equation
\begin{equation}
  \begin{split}
  \label{eq:translation_SGVB}
  \mathcal{L}^{SGVB}(\bm{\theta}, \bm{\varphi}| \bm{x}, \bm{y}) = (\frac{1}{S} \sum_{s=1}^S \log(\mathcal{N}(\bm{x} | \bm{\mu}_{\bm{\theta}_{\bm{x}}}(\bm{z}), \bm{\sigma}^2_{\bm{\theta}_{\bm{x}}}(\bm{z}))) + \log(\mathcal{N}(\bm{y} | \bm{\mu}_{\bm{\theta}_{\bm{y}}}(\bm{z}), \bm{\sigma}^2_{\bm{\theta}_{\bm{y}}}(\bm{z})))) + \\ \frac{1}{2} \sum_{j=1}^J(1 + \log((\bm{\sigma}_{\bm{\varphi}}(\bm{x}, \bm{y})_j)^2) - (\bm{\mu}_{\bm{\varphi}}(\bm{x}, \bm{y})_j^2 - (\bm{\sigma}_{\bm{\varphi}}(\bm{x}, \bm{y})_j^2))
  \end{split}
\end{equation}
where $j$ goes over all of the elements of
the mean and the diagonal covariance vector. The
second expression on the right is the analytical form of $-\KL{q_{\bm{\varphi}}(\bm{z} | \bm{x}, \bm{y})}{p(\bm{z})}$ \cite{kingma_auto-encoding_2013}.
\subsection{Training}
Variational Autoencoders in NLP have a tendency to collapse the KL term in the
SGVB objective to zero, effectively being reduced a RNNLM. As the KL can be seen
as a measure of how much information is encoded in $\bm{z}$, the KL-collapse makes the
use of VAE's to model language-agnostic features in the latent space useless.
One of the objectives is to make sure this KL-collapse doesn't happen.

We use KL-annealing where we anneal the ELBO by a pseudo-objective
\begin{equation}
  \label{eq:KL_annealing_ELBO}
  ELBO_{\beta_t} = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \beta_t * \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}
where $\beta_t$ is an annealing term going from $0$ to $1$ as we train. We do
this by choosing a warm-up $\beta$ and let $\beta_t = \min(1, \frac{t}{\beta})$ \cite{bowman_generating_2015},
letting this warm up be done linearly until we recover the original ELBO
objective, similar to normal simulated annealing \cite{Kirkpatrick1983}.

The training is then carried out using ADAM on the objective
$\mathcal{L}^{SGVB}$ where we take into account the KL-annealing. In order to
calculate the gradients efficiently we use Theano which has automatic support
for AutoDiff, laid out in section \ref{ch:autodiff}.