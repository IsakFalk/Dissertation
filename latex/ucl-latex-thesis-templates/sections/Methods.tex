\chapter{Methods and Theory}
\label{MethodsCh}

\section{ELBO}

Ideally we would like to optimize the log-likelihood of the data,
$\ell(\bm{\theta} ; \mathcal{D})$. For many models, it is not possible to
evaluate this quantity directly due to it being computationally and/or
analytically intractable.

Instead we make the following observation, letting $\mathcal{D} = \{\bm{x}_i\}_{i=1}^N$
\begin{equation}
  \label{eq:log_likelihood_sum}
  \log p_{\bm{\theta}}(\mathcal{D}) = \sum_{i=1}^N \log p_{\bm{\theta}}(\bm{x}_i)
\end{equation}
and looking at each term in the sum we can see that we may rewrite this as
\begin{align*}
  \log p_{\bm{\theta}}(\bm{x}_i) & = \log \int_{\mathcal{Z}} p_{\bm{\theta}}(\bm{z}, \bm{x}_i) \dif \bm{z}\\
                               & = \log \int_{\mathcal{Z}}q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)  \frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)} \dif \bm{z} \\
                               & = \log \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\right] \\
                               & \geq \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log \frac{p_{\bm{\theta}}(\bm{x}_i, \bm{z})}{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log \frac{p_{\bm{\theta}}(\bm{x}_i | z) p_{\bm{\theta}}(\bm{z})}{q_{\bm{\varphi}(\bm{z} | \bm{x}_i)}}\right] \\
                               & = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}\left[\log p_{\bm{\theta}}(\bm{x}_i | \bm{z})] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x}_i)}{p_{\bm{\theta}}(\bm{z})}\right]
\end{align*}
Which leads to the following observation, if we call the lower bound
$\mathcal{L}(\bm{\theta}, \bm{\varphi}; \bm{x})$, that
\begin{equation}
  \label{eq:ELBO_inequality}
  \log p_{\bm{\theta}}(\bm{x}) \geq \mathcal{L}(\bm{\theta}, \bm{\varphi}; \bm{x}) = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}.

In general this lower bound is still not tractable, however appealing to the
Strong Law of Large Numbers (SLLN) we can sample $\bm{z}^{l}$ and use the
reparametrisation trick (Include how the reparametrisation trick works) to approximate the lower bound by
\begin{equation}
  \label{eq:SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = \frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x}, \bm{z}) - \log q_{\bm{\varphi}}(\bm{z} | \bm{x})
\end{equation}.
This can also be expressed in the closed form of the KL so
\begin{equation}
  \label{eq:SGVB_analytical_KL}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = (\frac{1}{L} \sum_{l = 1}^L \log p_{\bm{\theta}}(\bm{x} | \bm{z})) - \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}

Since all quantities in this form have analytically known forms, it is simple to
evaluate and sample from $\tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x})$.

\section{Models}

The generative model follows the one laid out in
\cite{kingma_auto-encoding_2013}:

\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z] (x) {$\bm{x}$} ;
    \node[const, right=of z] (thetax){$\bm{\theta}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {thetax} {x} ;

    % Make plate
    \plate {zx} {(z)(x)} {$N$};
    
  \end{tikzpicture}
\end{figure}

where $\bm{z}$ is the latent variable representing the latent sentence structure
and the joint factorizes as $p_{\bm{\theta}}(\bm{x}, \bm{z}) =
p_{\bm{\theta}}(\bm{x} | \bm{z})p(\bm{z})$. In all models we let $p(\bm{z}) =
\mathcal{N}(\bm{z}| \bm{0}, \bm{I}_{D \times D})$.

\subsection{Reconstruction}

The reconstruction model follows the original model. We assume a latent variable
$\bm{z}$ that generates sentences $\bm{x}$ belonging to a language $\lang{X}$.

The ELBO and related calculations follow the ones laid out in the ELBO section.

\subsection{Translation}

Translation adds and extra observed variable $\bm{y}$, a sentence belonging to
$\lang{Y}$. The probabilistic model looks like:

\begin{figure}[H]
  \center
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (z) {$\bm{z}$} ;
    \node[obs, below=of z, xshift=-1.0cm] (x) {$\bm{x}$} ;
    \node[obs, below=of z, xshift=1.0cm] (y) {$\bm{y}$} ;
    \node[const, left=of z, xshift=-0.5cm] (thetax) {$\bm{\theta}_{\bm{x}}$} ;
    \node[const, right=of z, xshift=0.5cm] (thetay) {$\bm{\theta}_{\bm{y}}$} ;
    
    % Connect the nodes
    \edge {z} {x} ;
    \edge {z} {y} ;
    \edge {thetax} {x} ;
    \edge {thetay} {y} ;

    % Make plate
    \plate {zxy} {(z)(x)(y)} {$N$} ;
    
  \end{tikzpicture}
\end{figure}

From the graphical model we see that the joint distribution factorises such that
\begin{equation}
  \label{eq:joint_generative}
  p(\bm{\bm{z}}, \bm{x}, \bm{y}) = p(\bm{z})p(\bm{x} | \bm{z})p(\bm{y} | \bm{z}) = \mathcal{N}(0, I)\mathcal{N}(\mu_{\bm{\theta}_{\bm{x}}}(\bm{z}), \sigma_{\bm{\theta}_{\bm{x}}}(\bm{z}))\mathcal{N}(\mu_{\bm{\theta}_{\bm{y}}}(\bm{z}), \sigma_{\bm{\theta}_{\bm{y}}}(\bm{z}))
\end{equation}.
The notation $\mathcal{N}(\mu_{\bm{\theta}}(\bm{z}), \sigma_{\bm{\theta}}(\bm{z}))$
means that the mean and variance diagonal of the normal are represented by a
functional relationship $f: \bm{z} \mapsto (\mu, \sigma)$ where $f$ is a function
represented a a learnable neural network with parameters $\bm{\theta}$.

The recognition model is parametrised by the probability distribution
$q_{\varphi}(z | x, y)$. We are free to choose this however we like, but the closer
this is to the actual conditional distribution over the latent, $p(z | x, y)$
the tighter the ELBO inequality $\ell(x, y) \geq \mathcal{L}(x, y)$ will be. We
let the distributions all be gaussians to have analytical tractability
\begin{align*}
  q(z | x, y) & = q(z | x)q(z | y) \\
              & = N(\mu(x), \sigma(x) I) N(\mu(y), \sigma(y) I)
\end{align*}
where both normals defined by the output of neural networks.

Since the product of Guassian pdf's are gaussian itself, we have that may write
that
\begin{equation*}
  q(\bm{z} | \bm{x}, \bm{y}) = \mathcal{N}(\bm{z} | \mu(\bm{x}, \bm{y}), \sigma(\bm{x}, \bm{y}))
\end{equation*} which reduces to the expressions
\ref{eq:joint_indep_normal_mean} for the mean and
\ref{eq:joint_indep_normal_covariance} for the covariance.
Pulling all of these statements together, we get that the SGVB of the
translation case reduces to the equation
\begin{equation}
  \begin{split}
  \label{eq:translation_SGVB}
  \tilde{\mathcal{L}}^A(\bm{\theta}, \bm{\varphi}; \bm{x}) = (\frac{1}{L} \sum_{l=1}^L \log(\mathcal{N}(\bm{x} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z}))) + \log(\mathcal{N}(\bm{y} | \mu_{\bm{\theta}_y}(\bm{z}), \sigma_{\bm{\theta}_y}(\bm{z})))) + \\ \frac{1}{2} \sum_{j=1}^J(1 + \log((\sigma_j^{(i)})^2) - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2))
  \end{split}
\end{equation}

where the $\mu, \sigma$ is from the normal $\mathcal{N}(\bm{z} | \mu(\bm{x},
\bm{y}), \sigma(\bm{x}, \bm{y}))$.

\subsection{Training}
Variational Autoencoders in NLP have a tendency to collapse the KL term in the SGVB
objective to zero, effectively being reduced a RNNLM (CITE PAPER). As the KL can
be seen as a measure of how much information is encoded in $\bm{z}$ this event
makes the use of VAE's to model language-agnostic features in the latent space
useless. One of the objectives is to make sure this KL-collapse doesn't happen.

Following the tips laid out in (CITE PAPER VAE FOR NLP) we use KL-annealing
where we anneal the ELBO by a pseudo-objective
\begin{equation}
  \label{eq:KL_annealing_ELBO}
  ELBO_{\beta_t} = \E_{q_{\bm{\varphi}}(\bm{z} | \bm{x})}[\log p_{\bm{\theta}}(\bm{x} | \bm{z})] - \beta_t * \KL{q_{\bm{\varphi}}(\bm{z} | \bm{x})}{p_{\bm{\theta}}(\bm{z})}
\end{equation}
where $\beta_t$ is an annealing term going from $0$ to $1$ as we train. We do
this by choosing a warm-up $\beta$ and let $\beta_t = \min(1, \frac{t}{\beta})$,
letting this warm up be done linearly until we recover the original ELBO
objective, similar to normal simulated annealing (CITE ANNEALING PAPER).

