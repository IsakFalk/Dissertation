\chapter{Experiments}
\label{ExperimentsCh}

We compare how the different models fare for reconstruction and translation. 

\section{Data}

\subsection{Dataset}

We evaluate the models on two different datasets.

\begin{description}
\item[Europarl] A parallel corpus consisting of the proceedings of the European
  Parliament,
  comprising in total the 11 official languages of the European Union
  \cite{koehn2005epc}. We use the French-English language pair and preprocess
  it. The English dataset used for reconstruction has $625213$ train sentences
  and $69469$ test sentences. For the French-English dataset used for
  translation has $534635$ train sentences and $59404$ test sentences. The
  reason why the reconstruction dataset is bigger than the translation dataset
  is because for translation we take the intersection of the sentences that is
  between $2$ and $20$ words long for French and English, while in
  reconstruction we only need to consider the English sentences on their own.
\item[Custom] Our custom dataset is made from merging three different datasets;
  Europarl \cite{koehn2005epc}, News Commentary and UN corpus
  \cite{ZIEMSKI16.1195} \footnote{Taken from the wmt14 translation challenge: \texttt{http://www.statmt.org/wmt14/translation-task.html}}. This dataset is
  considerably bigger than the Europarl dataset. We only use it on the 
  twin-language model. After preprocessing the dataset it has $4268791$ train
  sentences and $474311$ test sentences.
\end{description}

\subsection{Preprocessing}

The raw data is unfit for use directly with the model. For one thing the raw
data is in the form of strings and in order to use neural networks we need to
transform this into a form using vectors in some space $\mathbb{R}^m$.

We do the following preprocessing steps:

\begin{itemize}
\item We specify the maximum and minimum length of the words. This enables us to
  pad sentences to be the same length.
\item We calculate the word frequencies in order to sort all of the words in the
  dataset in terms of how often it appear in absolute terms. This is then used
  to only retain the 30000 most common words. Words which are not part of this
  list gets replaced by an \texttt{<UNK>} token, specifying that it's an unknown
  word outside of the dictionary.
\item Newline characters were removed and replaced by \texttt{<EOS>},
  end-of-sentence tokens, signifying the end of a sentence.
\item When preprocessing, we let the preprocessed data from English and French
  be the intersection of the sentences in English and French that pass the above
  criteria.
\item Unicode is needed for French due to diacretics and accents which are not
  included in the normal ASCII encoder-decoder scheme for characters. We make
  sure that encoding and decoding is done correctly.
\end{itemize}

It is important to note that due to how languages differ, even though the
dataset might consist of sentence pairs this will still not mean that in general
both of the languages will have the same dictionary of words. Partially this is
due to the different ways that languages are built up when expressing meaning,
but on a more basic level, there are no bijection between languages as words
have slightly different meaning and contexts, with some words only existing in
one language but not the other. Similarly the sentence length of the same
sentence in English and French will in general be different.

\section{software}
All of the models were trained using Theano \cite{2016arXiv160502688short} and Lasagne \cite{lasagne} built on
top of the Python (2.7) programming language. All of the plots were
generated using Matplotlib \cite{Hunter:2007} and Seaborn \cite{michael_waskom_2014_12710}. Data processing was done with Numpy
\cite{Walt:2011:NAS:1957373.1957466} and Pandas \cite{mckinney-proc-scipy-2010}. The software repository can be found at \texttt{http://www.github.com/isakfalk/project}.

\section{Design}
Each model needs to specify a recognition model $q_{\bm{\varphi}}(\bm{z} |
\bm{x})$ and a generative model $p_{\bm{\theta}}(\bm{x} | \bm{z})$. Below we
specify all of the different models we trained and the hyperparameters we chose.

\subsection{Reconstruction}
We let the latent dimension be $50$ and the embedding dimension $300$. For
Reconstruction we only use the Europarl dataset. These dimensions were tuned by
evaluating performance over short runs.

\subsubsection{Generative model}
As specified before we let the prior be a unit normal:
\begin{equation}
  \label{eq:prior_distribution_generative}
  p(\bm{z}) = \mathcal{N}(\bm{z} | \bm{0}, \bm{I}).
\end{equation}

The generative model of the observed variable $p_{\bm{\theta}}(\bm{x} | \bm{z})$
is chosen to be the output from the WaveNet, as specified in \ref{ch:WaveNet}.
The distribution of each word is parametrised as a categorical
distribution dependent on all previously outputted probability vectors for the
preceding words and the latent vector $\bm{z}$.

\subsubsection{Recognition model}
We let the recognition model for all reconstruction models be a normal distribution
\begin{equation}
  \label{eq:rec_model_reconstruction}
  q_{\bm{\varphi}}(\bm{z} | \bm{x}) \sim \mathcal{N}(\bm{z}| \bm{\mu_{\bm{\varphi}}}(\bm{x}),
\bm{\sigma}_{\bm{\varphi}}(\bm{x})),
\end{equation}
with the parameters of the Gaussian being the output from different neural
networks depending on the model such that $\bm{\varphi}$ are the parameters of
the neural network. We train three different models.



